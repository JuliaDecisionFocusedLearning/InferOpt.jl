var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API reference","title":"API reference","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/#API-Reference","page":"API reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"InferOpt","category":"page"},{"location":"api/#InferOpt","page":"API reference","title":"InferOpt","text":"InferOpt\n\nA toolbox for using combinatorial optimization algorithms within machine learning pipelines.\n\nSee our preprint https://arxiv.org/abs/2207.13513\n\n\n\n\n\n","category":"module"},{"location":"api/#Types","page":"API reference","title":"Types","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [InferOpt]\nOrder   = [:type]","category":"page"},{"location":"api/#InferOpt.AbstractLayer","page":"API reference","title":"InferOpt.AbstractLayer","text":"AbstractLayer\n\nSupertype for all the layers defined in InferOpt.\n\nAll of these layers are callable, and differentiable with any ChainRules-compatible autodiff backend.\n\nInterface\n\n(layer::AbstractLayer)(args...; kwargs...)\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.AbstractLossLayer","page":"API reference","title":"InferOpt.AbstractLossLayer","text":"AbstractLossLayer <: AbstractLayer\n\nSupertype for all the loss layers defined in InferOpt.\n\nDepending on the precise loss, the arguments to the layer might vary\n\nInterface\n\n(layer::AbstractLossLayer)(θ; kwargs...) or\n(layer::AbstractLossLayer)(θ, θ_true; kwargs...) or\n(layer::AbstractLossLayer)(θ, y_true; kwargs...) or\n(layer::AbstractLossLayer)(θ, (; θ_true, y_true); kwargs...)\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.AbstractOptimizationLayer","page":"API reference","title":"InferOpt.AbstractOptimizationLayer","text":"AbstractOptimizationLayer <: AbstractLayer\n\nSupertype for all the optimization layers defined in InferOpt.\n\nInterface\n\n(layer::AbstractOptimizationLayer)(θ::AbstractArray; kwargs...)\ncompute_probability_distribution(layer, θ; kwargs...) (only if the layer is probabilistic)\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.AbstractPerturbation","page":"API reference","title":"InferOpt.AbstractPerturbation","text":"abstract type AbstractPerturbation <: Distributions.Distribution{Distributions.Univariate, Distributions.Continuous}\n\nAbstract type for a perturbation. It's a function that takes a parameter θ and returns a perturbed parameter by a distribution perturbation_dist.\n\nwarning: Warning\nAll subtypes should implement a perturbation_dist field, which is a ContinuousUnivariateDistribution.\n\nExisting implementations\n\nAdditivePerturbation\nMultiplicativePerturbation\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.AbstractRegularized","page":"API reference","title":"InferOpt.AbstractRegularized","text":"AbstractRegularized <: AbstractOptimizationLayer\n\nConvex regularization perturbation of a black box linear (in θ) optimizer\n\nŷ(θ) = argmax_{y ∈ C} {θᵀg(y) + h(y) - Ω(y)}\n\nwith g and h functions of y.\n\nInterface\n\n(regularized::AbstractRegularized)(θ; kwargs...): return ŷ(θ)\ncompute_regularization(regularized, y): return `Ω(y)\nget_maximizer(regularized): return the associated optimizer\n\nAvailable implementations\n\nSoftArgmax\nSparseArgmax\nSoftRank\nRegularizedFrankWolfe\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.AdditivePerturbation","page":"API reference","title":"InferOpt.AdditivePerturbation","text":"struct AdditivePerturbation{F}\n\nAdditive perturbation: θ ↦ θ + εZ, where Z is a random variable following perturbation_dist.\n\nFields\n\nperturbation_dist::Any: base distribution for the perturbation\nε::Float64: perturbation size\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.AdditivePerturbation-Tuple{AbstractArray}","page":"API reference","title":"InferOpt.AdditivePerturbation","text":"Apply the additive perturbation to the parameter θ.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.ExponentialOf","page":"API reference","title":"InferOpt.ExponentialOf","text":"Data structure modeling the exponential of a continuous univariate random variable.\n\nRandom.rand and Distributions.logpdf are defined for the ExponentialOf distribution.\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.FenchelYoungLoss","page":"API reference","title":"InferOpt.FenchelYoungLoss","text":"struct FenchelYoungLoss{O<:InferOpt.AbstractOptimizationLayer} <: InferOpt.AbstractLossLayer\n\nFenchel-Young loss associated with a given optimization layer.\n\nL(θ, y_true) = (Ω(y_true) - θᵀy_true) - (Ω(ŷ) - θᵀŷ)\n\nReference: https://arxiv.org/abs/1901.02324\n\nFields\n\noptimization_layer::AbstractOptimizationLayer: optimization layer that can be formulated as ŷ(θ) = argmax {θᵀy - Ω(y)} (either regularized or perturbed)\n\nCompatibility\n\nThis loss is compatible with:\n\nLinearMaximizer-based layers.\nPerturbedOracle layers, with additive or multiplicative perturbations (generic perturbations are not supported).\nany AbstractRegularized layer.\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.FenchelYoungLoss-Tuple{AbstractArray, AbstractArray}","page":"API reference","title":"InferOpt.FenchelYoungLoss","text":"Compute L(θ, y_true).\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.Fix1Kwargs","page":"API reference","title":"InferOpt.Fix1Kwargs","text":"struct Fix1Kwargs{F, K, T} <: Function\n\nCallable struct that fixes the first argument of f to x, and the keyword arguments to kwargs....\n\nFields\n\nf::Any: function\nx::Any: fixed first argument\nkwargs::Any: fixed keyword arguments\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.FixFirst","page":"API reference","title":"InferOpt.FixFirst","text":"struct FixFirst{F, T}\n\nCallable struct that fixes the first argument of f to x. Compared to Base.Fix1, works on functions with more than two arguments.\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.FixKwargs","page":"API reference","title":"InferOpt.FixKwargs","text":"struct FixKwargs{F, K}\n\nCallable struct that fixes the keyword arguments of f to kwargs..., and only accepts positional arguments.\n\nFields\n\nf::Any: function\nkwargs::Any: fixed keyword arguments\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.IdentityRelaxation","page":"API reference","title":"InferOpt.IdentityRelaxation","text":"IdentityRelaxation <: AbstractOptimizationLayer\n\nNaive relaxation of a black-box optimizer where constraints are simply forgotten.\n\nConsider (centering and) normalizing θ before applying it.\n\nFields\n\nmaximizer: underlying argmax function\n\nReference: https://arxiv.org/abs/2205.15213\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.ImitationLoss","page":"API reference","title":"InferOpt.ImitationLoss","text":"ImitationLoss <: AbstractLossLayer\n\nGeneric imitation loss of the form\n\nL(θ, t_true) = max_y {δ(y, t_true) + α θᵀ(y - y_true) - (Ω(y) - Ω(y_true))}\n\nWhen δ is zero, this is equivalent to a FenchelYoungLoss.\nWhen Ω is zero, this is equivalent to a StructuredSVMLoss.\n\nNote: by default, t_true is a named tuple with field y_true, but it can be any data structure for which the get_y_true method is implemented.\n\nFields\n\naux_loss_maximizer: function of (θ, t_true, α) that computes the argmax in the problem above\nδ: base loss function\nΩ: regularization function\nα::Float64: hyperparameter with a default value of 1.0\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.ImitationLoss-Tuple{AbstractArray, Any}","page":"API reference","title":"InferOpt.ImitationLoss","text":"(il::ImitationLoss)(θ, t_true; kwargs...)\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.ImitationLoss-Tuple{}","page":"API reference","title":"InferOpt.ImitationLoss","text":"ImitationLoss(; aux_loss_maximizer, δ, Ω, α=1.0)\n\nExplicit constructor with keyword arguments.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.Interpolation","page":"API reference","title":"InferOpt.Interpolation","text":"Interpolation <: AbstractOptimizationLayer\n\nPiecewise-linear interpolation of a black-box optimizer.\n\nFields\n\nmaximizer: underlying argmax function\nλ::Float64: smoothing parameter (smaller = more faithful approximation, larger = more informative gradients)\n\nReference: https://arxiv.org/abs/1912.02175\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.LinearMaximizer","page":"API reference","title":"InferOpt.LinearMaximizer","text":"struct LinearMaximizer{F, G, H}\n\nWrapper for generic minear maximizers of the form argmax_y θᵀg(y) + h(y). It is compatible with the following layers\n\nPerturbedAdditive (with or without a FenchelYoungLoss)\nPerturbedMultiplicative (with or without a FenchelYoungLoss)\nSPOPlusLoss\n\nFields\n\nmaximizer::Any: function θ ⟼ argmax_y θᵀg(y) + h(y)\ng::Any: function g(y) used in the objective\nh::Any: function h(y) used in the objective\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.LinearMaximizer-Tuple{AbstractArray}","page":"API reference","title":"InferOpt.LinearMaximizer","text":"Calls the wrapped maximizer.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.LinearMaximizer-Tuple{Any}","page":"API reference","title":"InferOpt.LinearMaximizer","text":"LinearMaximizer(\n    maximizer;\n    g,\n    h\n) -> LinearMaximizer{_A, typeof(InferOpt.identity_kw), ComposedFunction{typeof(zero), typeof(InferOpt.eltype_kw)}} where _A\n\n\nConstructor for LinearMaximizer.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.MultiplicativePerturbation","page":"API reference","title":"InferOpt.MultiplicativePerturbation","text":"MultiplicativePerturbation(\n    perturbation_dist,\n    ε\n) -> InferOpt.MultiplicativePerturbation\nMultiplicativePerturbation(\n    perturbation_dist,\n    ε,\n    shift\n) -> InferOpt.MultiplicativePerturbation\n\n\nConstructor for MultiplicativePerturbation.\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.MultiplicativePerturbation-2","page":"API reference","title":"InferOpt.MultiplicativePerturbation","text":"struct MultiplicativePerturbation{F}\n\nMultiplicative perturbation: θ ↦ θ ⊙ exp(εZ - shift)\n\nFields\n\nperturbation_dist::Any: base distribution for the perturbation\nε::Float64: perturbation size\nshift::Float64: optional shift to have 0 mean, default value is ε²/2\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.MultiplicativePerturbation-Tuple{AbstractArray}","page":"API reference","title":"InferOpt.MultiplicativePerturbation","text":"Apply the multiplicative perturbation to the parameter θ.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.NormalAdditiveGradLogdensity","page":"API reference","title":"InferOpt.NormalAdditiveGradLogdensity","text":"struct NormalAdditiveGradLogdensity\n\nMethod with parameters to compute the gradient of the logdensity of η = θ + εZ w.r.t. θ., with Z ∼ N(0, 1).\n\nFields\n\nε::Float64: perturbation size\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.NormalAdditiveGradLogdensity-Tuple{AbstractArray, AbstractArray}","page":"API reference","title":"InferOpt.NormalAdditiveGradLogdensity","text":"Compute the gradient of the logdensity of η = θ + εZ w.r.t. θ., with Z ∼ N(0, 1).\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.NormalMultiplicativeGradLogdensity","page":"API reference","title":"InferOpt.NormalMultiplicativeGradLogdensity","text":"struct NormalMultiplicativeGradLogdensity\n\nMethod with parameters to compute the gradient of the logdensity of η = θ ⊙ exp(εZ - shift) w.r.t. θ., with Z ∼ N(0, 1).\n\nFields\n\nε::Float64: perturbation size\nshift::Float64: optional shift to have 0 mean\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.NormalMultiplicativeGradLogdensity-Tuple{AbstractArray, AbstractArray}","page":"API reference","title":"InferOpt.NormalMultiplicativeGradLogdensity","text":"Compute the gradient of the logdensity of η = θ ⊙ exp(εZ - shift) w.r.t. θ., with Z ∼ N(0, 1).\n\nwarning: Warning\nη should be a realization of θ, i.e. should be of the same sign.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.PerturbedOracle","page":"API reference","title":"InferOpt.PerturbedOracle","text":"struct PerturbedOracle{D, F, t, variance_reduction, G, R, S} <: InferOpt.AbstractOptimizationLayer\n\nDifferentiable perturbation of a black box optimizer of type F, with perturbation of type D.\n\nThis struct is as wrapper around Reinforce from DifferentiableExpectations.jl.\n\nThere are three different available constructors that behave differently in the package:\n\nPerturbedOracle\nPerturbedAdditive\nPerturbedMultiplicative\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.PerturbedOracle-Tuple{AbstractArray}","page":"API reference","title":"InferOpt.PerturbedOracle","text":"Forward pass of the perturbed optimizer.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.PerturbedOracle-Tuple{Any, Any}","page":"API reference","title":"InferOpt.PerturbedOracle","text":"PerturbedOracle(\n    maximizer,\n    dist_constructor;\n    dist_logdensity_grad,\n    nb_samples,\n    variance_reduction,\n    threaded,\n    seed,\n    rng,\n    kwargs...\n) -> PerturbedOracle{_A, _B, _C, _D, Nothing, Random.TaskLocalRNG, Nothing} where {_A, _B, _C, _D}\n\n\nConstructor for PerturbedOracle.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.Pushforward","page":"API reference","title":"InferOpt.Pushforward","text":"struct Pushforward{O<:InferOpt.AbstractOptimizationLayer, P} <: InferOpt.AbstractLayer\n\nDifferentiable pushforward of a probabilistic optimization layer with an arbitrary function post-processing function.\n\nPushforward can be used for direct regret minimization (aka learning by experience) when the post-processing returns a cost.\n\nFields\n\noptimization_layer::InferOpt.AbstractOptimizationLayer: probabilistic optimization layer\npost_processing::Any: callable\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.Pushforward-Tuple{AbstractArray}","page":"API reference","title":"InferOpt.Pushforward","text":"Output the expectation of pushforward.post_processing(X), where X follows the distribution defined by pushforward.optimization_layer applied to θ.\n\nThis function is differentiable, even if pushforward.post_processing isn't.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.RegularizedFrankWolfe","page":"API reference","title":"InferOpt.RegularizedFrankWolfe","text":"RegularizedFrankWolfe <: AbstractRegularized\n\nRegularized optimization layer which relies on the Frank-Wolfe algorithm to define a probability distribution while solving\n\nŷ(θ) = argmax_{y ∈ C} {θᵀy - Ω(y)}\n\nwarning: Warning\nSince this is a conditional dependency, you need to have loaded the following packages before using RegularizedFrankWolfe:DifferentiableFrankWolfe.jl\nFrankWolfe.jl\nImplicitDifferentiation.jl\n\nFields\n\nlinear_maximizer: linear maximization oracle θ -> argmax_{x ∈ C} θᵀx, implicitly defines the polytope C\nΩ: regularization function Ω(y)\nΩ_grad: gradient function of the regularization function ∇Ω(y)\nfrank_wolfe_kwargs: named tuple of keyword arguments passed to the Frank-Wolfe algorithm\nimplicit_kwargs: named tuple of keyword arguments passed to the implicit differentiation algorithm (in particular, the needed linear solver)\n\nFrank-Wolfe parameters\n\nSome values you can tune:\n\nepsilon::Float64: precision target\nmax_iteration::Integer: max number of iterations\ntimeout::Float64: max runtime in seconds\nlazy::Bool: caching strategy\naway_steps::Bool: avoid zig-zagging\nline_search::FrankWolfe.LineSearchMethod: step size selection\nverbose::Bool: console output\n\nSee the documentation of FrankWolfe.jl for details.\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.RegularizedFrankWolfe-Tuple{AbstractArray}","page":"API reference","title":"InferOpt.RegularizedFrankWolfe","text":"(regularized::RegularizedFrankWolfe)(θ; kwargs...)\n\nApply compute_probability_distribution(regularized, θ; kwargs...) and return the expectation.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.SPOPlusLoss","page":"API reference","title":"InferOpt.SPOPlusLoss","text":"struct SPOPlusLoss{F} <: InferOpt.AbstractLossLayer\n\nConvex surrogate of the Smart \"Predict-then-Optimize\" loss.\n\nFields\n\nmaximizer::Any: linear maximizer function of the form θ -> ŷ(θ) = argmax θᵀy\nα::Float64: convexification parameter, default = 2.0\n\nReference: https://arxiv.org/abs/1710.08005\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.SPOPlusLoss-Tuple{AbstractArray, AbstractArray, AbstractArray}","page":"API reference","title":"InferOpt.SPOPlusLoss","text":"Forward pass of the SPO+ loss with given target θ_true and y_true. The third argument y_true is optional, as it can be computed from θ_true. However, providing it directly can save computation time.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.SPOPlusLoss-Tuple{AbstractArray, AbstractArray}","page":"API reference","title":"InferOpt.SPOPlusLoss","text":"Forward pass of the SPO+ loss with given target θ_true. For better performance, you can also provide y_true directly as a third argument.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.SPOPlusLoss-Tuple{Any}","page":"API reference","title":"InferOpt.SPOPlusLoss","text":"SPOPlusLoss(maximizer; α) -> SPOPlusLoss\n\n\nConstructor for SPOPlusLoss.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.SoftArgmax","page":"API reference","title":"InferOpt.SoftArgmax","text":"SoftArgmax <: Regularized\n\nSoft argmax activation function s(z) = (e^zᵢ / ∑ e^zⱼ)ᵢ.\n\nCorresponds to regularized prediction on the probability simplex with entropic penalty.\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.SoftRank","page":"API reference","title":"InferOpt.SoftRank","text":"SoftRank{is_l2_regularized} <: AbstractRegularized\n\nFast differentiable ranking regularized layer. It uses an L2 regularization if is_l2_regularized is true, else it uses an entropic (kl) regularization.\n\nAs an AbstractRegularized layer, it can also be used for supervised learning with a FenchelYoungLoss.\n\nFields\n\nε::Float64: size of the regularization\nrev::Bool: rank in ascending order if false\n\nReference: https://arxiv.org/abs/2002.08871\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.SoftRank-Tuple{}","page":"API reference","title":"InferOpt.SoftRank","text":"SoftRank(; ε::Float64=1.0, rev::Bool=false, is_l2_regularized::Bool=true)\n\nConstructor for SoftRank.\n\nArguments\n\nε::Float64=1.0: size of the regularization\nrev::Bool=false: rank in ascending order if false\n`regularization=\"l2\": used regularization, either \"l2\" or \"kl\"\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.SoftSort","page":"API reference","title":"InferOpt.SoftSort","text":"SoftSort{is_l2_regularized} <: AbstractOptimizationLayer\n\nFast differentiable sorting optimization layer. It uses an L2 regularization if is_l2_regularized is true, else it uses an entropic (kl) regularization.\n\nReference https://arxiv.org/abs/2002.08871\n\nFields\n\nε::Float64: size of the regularization\nrev::Bool: sort in ascending order if false\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.SoftSort-Tuple{}","page":"API reference","title":"InferOpt.SoftSort","text":"SoftSort(; ε::Float64=1.0, rev::Bool=false, is_l2_regularized::Bool=true)\n\nConstructor for SoftSort.\n\nArguments\n\nε::Float64=1.0: size of the regularization\nrev::Bool=false: sort in ascending order if false\nis_l2_regularized::Bool=true: use l2 regularization if true, else kl regularization\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.SparseArgmax","page":"API reference","title":"InferOpt.SparseArgmax","text":"SparseArgmax <: AbstractRegularized\n\nCompute the Euclidean projection of the vector z onto the probability simplex.\n\nCorresponds to regularized prediction on the probability simplex with square norm penalty.\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.StructuredSVMLoss","page":"API reference","title":"InferOpt.StructuredSVMLoss","text":"StructuredSVMLoss <: AbstractLossLayer\n\nLoss associated with the Structured Support Vector Machine, defined by\n\nL(θ, y_true) = max_y {δ(y, y_true) + α θᵀ(y - y_true)}\n\nReference: http://www.nowozin.net/sebastian/papers/nowozin2011structured-tutorial.pdf (Chapter 6)\n\nFields\n\naux_loss_maximizer::M: function of (θ, y_true, α) that computes the argmax in the problem above\nδ::L: base loss function\nα::Float64: hyperparameter with a default value of 1.0\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.StructuredSVMLoss-Tuple{AbstractArray, AbstractArray}","page":"API reference","title":"InferOpt.StructuredSVMLoss","text":"(ssvml::StructuredSVMLoss)(θ, y_true; kwargs...)\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.StructuredSVMLoss-Tuple{}","page":"API reference","title":"InferOpt.StructuredSVMLoss","text":"StructuredSVMLoss(; aux_loss_maximizer, δ, α=1.0)\n\n\n\n\n\n","category":"method"},{"location":"api/#Functions","page":"API reference","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [InferOpt]\nOrder   = [:function]","category":"page"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, InferOpt.AbstractPerturbation}","page":"API reference","title":"Base.rand","text":"rand(\n    rng::Random.AbstractRNG,\n    perturbation::InferOpt.AbstractPerturbation\n) -> Any\n\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, InferOpt.ExponentialOf}","page":"API reference","title":"Base.rand","text":"rand(\n    rng::Random.AbstractRNG,\n    d::InferOpt.ExponentialOf\n) -> Any\n\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.logpdf-Tuple{InferOpt.ExponentialOf, Real}","page":"API reference","title":"Distributions.logpdf","text":"logpdf(d::InferOpt.ExponentialOf, x::Real) -> Any\n\n\nReturn the log-density of the ExponentialOf distribution at x. It is equal to logpdf(d log(x)) - log(x).\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.PerturbedAdditive-Tuple{Any}","page":"API reference","title":"InferOpt.PerturbedAdditive","text":"PerturbedAdditive(\n    maximizer;\n    ε,\n    perturbation_dist,\n    nb_samples,\n    variance_reduction,\n    seed,\n    threaded,\n    rng,\n    dist_logdensity_grad\n) -> Union{PerturbedOracle{InferOpt.AdditivePerturbation{Distributions.Normal{Float64}}, _A, _B, _C, Nothing, Random.TaskLocalRNG, Nothing} where {_A, _B, _C}, PerturbedOracle{InferOpt.AdditivePerturbation{Distributions.Normal{Float64}}, _A, _B, _C, InferOpt.NormalAdditiveGradLogdensity, Random.TaskLocalRNG, Nothing} where {_A, _B, _C}}\n\n\nConstructor for PerturbedOracle with an additive perturbation.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.PerturbedMultiplicative-Tuple{Any}","page":"API reference","title":"InferOpt.PerturbedMultiplicative","text":"PerturbedMultiplicative(\n    maximizer;\n    ε,\n    perturbation_dist,\n    nb_samples,\n    variance_reduction,\n    seed,\n    threaded,\n    rng,\n    dist_logdensity_grad\n) -> Union{PerturbedOracle{InferOpt.MultiplicativePerturbation{Distributions.Normal{Float64}}, _A, _B, _C, Nothing, Random.TaskLocalRNG, Nothing} where {_A, _B, _C}, PerturbedOracle{InferOpt.MultiplicativePerturbation{Distributions.Normal{Float64}}, _A, _B, _C, InferOpt.NormalMultiplicativeGradLogdensity, Random.TaskLocalRNG, Nothing} where {_A, _B, _C}}\n\n\nConstructor for PerturbedOracle with a multiplicative perturbation.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.ZeroOneImitationLoss","page":"API reference","title":"InferOpt.ZeroOneImitationLoss","text":"ZeroOneStructuredSVMLoss(α)\n\nImplementation of the ImitationLoss based on a 0-1 loss for multiclass classification with no regularization.\n\n\n\n\n\n","category":"function"},{"location":"api/#InferOpt.ZeroOneStructuredSVMLoss","page":"API reference","title":"InferOpt.ZeroOneStructuredSVMLoss","text":"ZeroOneStructuredSVMLoss\n\nImplementation of the StructuredSVMLoss based on a 0-1 loss for multiclass classification.\n\n\n\n\n\n","category":"function"},{"location":"api/#InferOpt.apply_g-Tuple{LinearMaximizer, Any}","page":"API reference","title":"InferOpt.apply_g","text":"apply_g(f::LinearMaximizer, y; kwargs...) -> Any\n\n\nApplies the function g of the LinearMaximizer f to y.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.compute_probability_distribution","page":"API reference","title":"InferOpt.compute_probability_distribution","text":"compute_probability_distribution(layer, θ; kwargs...)\n\nApply a probabilistic optimization layer to an objective direction θ in order to generate a FixedAtomsProbabilityDistribution on the vertices of a polytope.\n\n\n\n\n\n","category":"function"},{"location":"api/#InferOpt.compute_regularization","page":"API reference","title":"InferOpt.compute_regularization","text":"compute_regularization(regularized::AbstractRegularized, y)\n\nReturn the convex penalty Ω(y) associated with an AbstractRegularized layer.\n\n\n\n\n\n","category":"function"},{"location":"api/#InferOpt.get_y_true","page":"API reference","title":"InferOpt.get_y_true","text":"get_y_true(t_true::Any)\n\nRetrieve y_true from t_true.\n\nThis method should be implemented when using a custom data structure for t_true other than a NamedTuple.\n\n\n\n\n\n","category":"function"},{"location":"api/#InferOpt.get_y_true-Tuple{NamedTuple}","page":"API reference","title":"InferOpt.get_y_true","text":"get_y_true(t_true::NamedTuple)\n\nRetrieve y_true from t_true. t_true must contain an y_true field.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.half_square_norm-Tuple{AbstractArray}","page":"API reference","title":"InferOpt.half_square_norm","text":"half_square_norm(x::AbstractArray) -> Any\n\n\nCompute the squared Euclidean norm of x and divide it by 2.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.isproba-Tuple{Real}","page":"API reference","title":"InferOpt.isproba","text":"isproba(x::Real) -> Any\n\n\nCheck whether x ∈ [0,1].\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R<:Real","page":"API reference","title":"InferOpt.isprobadist","text":"isprobadist(p::AbstractArray{R<:Real, 1}) -> Any\n\n\nCheck whether the elements of p are nonnegative and sum to 1.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.objective_value-Tuple{LinearMaximizer, Any, Any}","page":"API reference","title":"InferOpt.objective_value","text":"objective_value(f::LinearMaximizer, θ, y; kwargs...) -> Any\n\n\nComputes the objective value of given LinearMaximizer f, knowing weights θ and solution y. i.e. θᵀg(y) + h(y)\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R<:Real","page":"API reference","title":"InferOpt.one_hot_argmax","text":"one_hot_argmax(\n    z::AbstractArray{R<:Real, 1};\n    kwargs...\n) -> Any\n\n\nOne-hot encoding of the argmax function.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.positive_part-Tuple{Any}","page":"API reference","title":"InferOpt.positive_part","text":"positive_part(x) -> Any\n\n\nCompute max(x, 0).\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.ranking-Tuple{AbstractVector}","page":"API reference","title":"InferOpt.ranking","text":"ranking(θ::AbstractVector; rev, kwargs...) -> Any\n\n\nCompute the vector r such that rᵢ is the rank of θᵢ in θ.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R<:Real","page":"API reference","title":"InferOpt.shannon_entropy","text":"shannon_entropy(p::AbstractArray{R<:Real, 1}) -> Any\n\n\nCompute the Shannon entropy of a probability distribution: H(p) = -∑ pᵢlog(pᵢ).\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.simplex_projection_and_support-Tuple{AbstractVector}","page":"API reference","title":"InferOpt.simplex_projection_and_support","text":"simplex_projection_and_support(z)\n\nCompute the Euclidean projection p of z on the probability simplex (also called sparse_argmax), and the indicators s of its support.\n\nReference: https://arxiv.org/abs/1602.02068.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.soft_rank-Tuple{AbstractVector}","page":"API reference","title":"InferOpt.soft_rank","text":"soft_rank(θ::AbstractVector; ε=1.0, rev::Bool=false)\n\nFast differentiable ranking of vector θ.\n\nArguments\n\nθ: vector to sort\n\nKeyword (optional) arguments\n\nε::Float64=1.0: size of the regularization\nrev::Bool=false: sort in ascending order if false\nregularization=:l2: use l2 regularization if :l2, and kl regularization if :kl\n\nSee also soft_rank_l2 and soft_rank_kl.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.soft_rank_kl-Tuple{AbstractVector}","page":"API reference","title":"InferOpt.soft_rank_kl","text":"soft_rank_kl(θ::AbstractVector; ε=1.0, rev::Bool=false)\n\nRank vector θ with kl regularization.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.soft_rank_l2-Tuple{AbstractVector}","page":"API reference","title":"InferOpt.soft_rank_l2","text":"soft_rank_l2(θ::AbstractVector; ε=1.0, rev::Bool=false)\n\nRank vector θ with l2 regularization.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.soft_sort-Tuple{AbstractVector}","page":"API reference","title":"InferOpt.soft_sort","text":"soft_sort(θ::AbstractVector; ε=1.0, rev::Bool=false, regularization=:l2)\n\nFast differentiable sort of vector θ.\n\nArguments\n\nθ: vector to sort\n\nKeyword (optional) arguments\n\nε::Float64=1.0: size of the regularization\nrev::Bool=false: sort in ascending order if false\nregularization=:l2: use l2 regularization if :l2, and kl regularization if :kl\n\nSee also soft_sort_l2 and soft_sort_kl.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.soft_sort_kl-Tuple{AbstractVector}","page":"API reference","title":"InferOpt.soft_sort_kl","text":"soft_sort_kl(θ::AbstractVector; ε=1.0, rev::Bool=false)\n\nSort vector θ with kl regularization.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.soft_sort_l2-Tuple{AbstractVector}","page":"API reference","title":"InferOpt.soft_sort_l2","text":"soft_sort_l2(θ::AbstractVector; ε=1.0, rev::Bool=false)\n\nSort vector θ with l2 regularization.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.zero_one_loss-Tuple{AbstractArray, AbstractArray}","page":"API reference","title":"InferOpt.zero_one_loss","text":"zero_one_loss(y, y_true)\n\n0-1 loss for multiclass classification: δ(y, y_true) = 0 if y = y_true, and 1 otherwise.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.zero_one_loss_maximizer-Union{Tuple{R}, Tuple{AbstractVector, AbstractVector{R}, Any}} where R<:Real","page":"API reference","title":"InferOpt.zero_one_loss_maximizer","text":"zero_one_loss_maximizer(y, y_true; α)\n\nFor δ = zero_one_loss, compute\n\nargmax_y {δ(y, y_true) + α θᵀ(y - y_true)}\n\n\n\n\n\n","category":"method"},{"location":"api/#Index","page":"API reference","title":"Index","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [InferOpt]","category":"page"},{"location":"advanced_applications/#Advanced-applications","page":"Advanced applications","title":"Advanced applications","text":"","category":"section"},{"location":"advanced_applications/","page":"Advanced applications","title":"Advanced applications","text":"More advanced applications can be found in the four following satellite packages, and their associated documentations:","category":"page"},{"location":"advanced_applications/","page":"Advanced applications","title":"Advanced applications","text":"WarcraftShortestPaths.jl: computing shortest paths on Warcraft maps\nStochasticVehicleScheduling.jl: learning pipeline to solve the Stochastic Vehicle Scheduling problem\nSingleMachineScheduling.jl: learn to solve the single machine scheduling problem\nMinimumWeightTwoStageSpanningTree.jl: learn to solve the two stage minimum weight spanning tree","category":"page"},{"location":"losses/#Losses","page":"Losses","title":"Losses","text":"","category":"section"},{"location":"losses/","page":"Losses","title":"Losses","text":"info: Work in progress\nCome back later!","category":"page"},{"location":"losses/","page":"Losses","title":"Losses","text":"using AbstractTrees, InferOpt, InteractiveUtils\nAbstractTrees.children(x::Type) = subtypes(x)\nprint_tree(InferOpt.AbstractLossLayer)","category":"page"},{"location":"background/#Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The goal of InferOpt.jl is to make machine learning pipelines more expressive by incorporating combinatorial optimization layers.","category":"page"},{"location":"background/#How-the-math-works","page":"Background","title":"How the math works","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Consider the following combinatorial optimization problem:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"    fcolon theta longmapsto arg max_v in mathcalV theta^top v","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"where mathcalV subset mathbbR^d is a finite set of feasible solutions, and theta is an objective vector. Note that any linear program (LP) or mixed integer linear program (MILP) can be formulated this way.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Unfortunately, the optimal solution f(theta) is a piecewise constant function of theta, which means its derivative is either zero or undefined. Starting with an oracle for f, InferOpt.jl approximates it with a differentiable \"layer\", whose derivatives convey meaningful slope information. Such a layer can then be used within a machine learning pipeline, and gradient descent will succeed. InferOpt.jl also provides adequate loss functions for structured learning.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"For more details on the theoretical aspects, you can check out our paper:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"note: Reference\nLearning with Combinatorial Optimization Layers: a Probabilistic Approach","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"For a broader perspective on the interactions between machine learning and combinatorial optimization, please refer to the following surveys:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"note: Reference\nMachine Learning for Combinatorial Optimization: A Methodological Tour d’Horizon","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"note: Reference\nEnd-to-end Constrained Optimization Learning: A Survey","category":"page"},{"location":"background/#How-the-code-works","page":"Background","title":"How the code works","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Since we want our package to be as generic as possible, we don't make any assumptions on the oracle used for f. That way, the best solver can be selected for each use case. We only ask the user to provide a black box function called maximizer, taking theta as argument and returning f(theta).","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"This function is then wrapped into a callable Julia struct, which can be used (for instance) within neural networks from the Flux.jl or Lux.jl library. To achieve this compatibility, we leverage Julia's automatic differentiation (AD) ecosystem, which revolves around the ChainRules.jl package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"EditURL = \"https://github.com/JuliaDecisionFocusedLearning/InferOpt.jl/blob/main/README.md\"","category":"page"},{"location":"#InferOpt.jl","page":"Home","title":"InferOpt.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage) (Image: Code Style: Blue) (Image: Aqua QA)","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"InferOpt.jl is a toolbox for using combinatorial optimization algorithms within machine learning pipelines.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It allows you to create differentiable layers from optimization oracles that do not have meaningful derivatives. Typical examples include mixed integer linear programs or graph algorithms.","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the stable version, open a Julia REPL and run the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(\"InferOpt\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"To install the development version, run this command instead:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(url=\"https://github.com/JuliaDecisionFocusedLearning/InferOpt.jl\")","category":"page"},{"location":"#Citing-us","page":"Home","title":"Citing us","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use our package in your research, please cite the following paper:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Learning with Combinatorial Optimization Layers: a Probabilistic Approach - Guillaume Dalle, Léo Baty, Louis Bouvier and Axel Parmentier (2022)","category":"page"},{"location":"#Related-packages","page":"Home","title":"Related packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following libraries implement similar functionalities:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ImplicitDifferentiation.jl: automatic differentiation of implicit functions \nDiffOpt.jl: differentiating convex optimization programs w.r.t. program parameters\nJAXopt: hardware accelerated, batchable and differentiable optimizers in JAX","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"EditURL = \"../../examples/tutorial.jl\"","category":"page"},{"location":"tutorial/#Basic-tutorial","page":"Basic tutorial","title":"Basic tutorial","text":"","category":"section"},{"location":"tutorial/#Context","page":"Basic tutorial","title":"Context","text":"","category":"section"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"Let us imagine that we observe the itineraries chosen by a public transport user in several different networks, and that we want to understand their decision-making process (a.k.a. recover their utility function).","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"More precisely, each point in our dataset consists in:","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"a graph G\na shortest path P from the top left to the bottom right corner","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"We don't know the true costs that were used to compute the shortest path, but we can exploit a set of features to approximate these costs. The question is: how should we combine these features?","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"We will use InferOpt.jl to learn the appropriate weights, so that we may propose relevant paths to the user in the future.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"using Flux\nusing Graphs\nusing GridGraphs\nusing InferOpt\nusing LinearAlgebra\nusing ProgressMeter\nusing Random\nusing Statistics\nusing Test\nusing UnicodePlots\n\nRandom.seed!(63);\nnothing #hide","category":"page"},{"location":"tutorial/#Grid-graphs","page":"Basic tutorial","title":"Grid graphs","text":"","category":"section"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"For the purposes of this tutorial, we consider grid graphs, as implemented in GridGraphs.jl. In such graphs, each vertex corresponds to a couple of coordinates (i j), where 1 leq i leq h and 1 leq j leq w.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"To ensure acyclicity, we only allow the user to move right, down or both. Since the cost of a move is defined as the cost of the arrival vertex, any grid graph is entirely characterized by its cost matrix theta in mathbbR^h times w.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"h, w = 50, 100\nqueen_directions = GridGraphs.QUEEN_DIRECTIONS_ACYCLIC\ng = GridGraph(rand(h, w); directions=queen_directions);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"For convenience, GridGraphs.jl also provides custom functions to compute shortest paths efficiently. Let us see what those paths look like.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"p = path_to_matrix(g, grid_topological_sort(g, 1, nv(g)));\nspy(p)","category":"page"},{"location":"tutorial/#Dataset","page":"Basic tutorial","title":"Dataset","text":"","category":"section"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"As announced, we do not know the cost of each vertex, only a set of relevant features. Let us assume that the user combines them using a shallow neural network.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"nb_features = 5\ntrue_encoder = Chain(Dense(nb_features, 1), z -> dropdims(z; dims=1));\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"The true vertex costs computed from this encoding are then used within shortest path computations. To be consistent with the literature, we frame this problem as a linear maximization problem, which justifies the change of sign in front of theta. Note that linear_maximizer can take keyword arguments, eg. to give additional information about the instance that θ doesn't contain.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"function linear_maximizer(θ; directions)\n    g = GridGraph(-θ; directions=directions)\n    path = grid_topological_sort(g, 1, nv(g))\n    return path_to_matrix(g, path)\nend;\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"We now have everything we need to build our dataset.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"nb_instances = 30\n\nX_train = [randn(Float32, nb_features, h, w) for n in 1:nb_instances];\nθ_train = [true_encoder(x) for x in X_train];\nY_train = [linear_maximizer(θ; directions=queen_directions) for θ in θ_train];\nnothing #hide","category":"page"},{"location":"tutorial/#Learning","page":"Basic tutorial","title":"Learning","text":"","category":"section"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"We create a trainable model with the same structure as the true encoder but another set of randomly-initialized weights.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"initial_encoder = Chain(Dense(nb_features, 1), z -> dropdims(z; dims=1));\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"Here is the crucial part where InferOpt.jl intervenes: the choice of a clever loss function that enables us to","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"differentiate through the shortest path maximizer, even though it is a combinatorial operation\nevaluate the quality of our model based on the paths that it recommends","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"layer = PerturbedMultiplicative(linear_maximizer; ε=0.1, nb_samples=5);\nloss = FenchelYoungLoss(layer);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"This probabilistic layer is just a thin wrapper around our linear_maximizer, but with a very different behavior:","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"p_layer = layer(θ_train[1]; directions=queen_directions);\nspy(p_layer)","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"Instead of choosing just one path, it spreads over several possible paths, allowing its output to change smoothly as theta varies. Thanks to this smoothing, we can now train our model with a standard gradient optimizer.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"encoder = deepcopy(initial_encoder)\nopt = Flux.Adam();\nopt_state = Flux.setup(opt, encoder)\nlosses = Float64[]\nfor epoch in 1:100\n    l = 0.0\n    for (x, y) in zip(X_train, Y_train)\n        grads = Flux.gradient(encoder) do m\n            l += loss(m(x), y; directions=queen_directions)\n        end\n        Flux.update!(opt_state, encoder, grads[1])\n    end\n    push!(losses, l)\nend;\nnothing #hide","category":"page"},{"location":"tutorial/#Results","page":"Basic tutorial","title":"Results","text":"","category":"section"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"Since the Fenchel-Young loss is convex, it is no wonder that optimization worked like a charm.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"lineplot(losses; xlabel=\"Epoch\", ylabel=\"Loss\")","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"To assess performance, we can compare the learned weights with their true (hidden) values","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"learned_weight = encoder[1].weight / norm(encoder[1].weight)\ntrue_weight = true_encoder[1].weight / norm(true_encoder[1].weight)\nhcat(learned_weight, true_weight)","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"We are quite close to recovering the exact user weights. But in reality, it doesn't matter as much as our ability to provide accurate path predictions. Let us therefore compare our predictions with the actual paths on the training set.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"normalized_hamming(x, y) = mean(x[i] != y[i] for i in eachindex(x));\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"Y_train_pred = [linear_maximizer(encoder(x); directions=queen_directions) for x in X_train];\n\ntrain_error = mean(\n    normalized_hamming(y, y_pred) for (y, y_pred) in zip(Y_train, Y_train_pred)\n)","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"Not too bad, at least compared with our random initial encoder.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"Y_train_pred_initial = [\n    linear_maximizer(initial_encoder(x); directions=queen_directions) for x in X_train\n];\n\ntrain_error_initial = mean(\n    normalized_hamming(y, y_pred) for (y, y_pred) in zip(Y_train, Y_train_pred_initial)\n)","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"This is definitely a success. Of course in real prediction settings we should measure performance on a test set as well. This is left as an exercise to the reader.","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"","category":"page"},{"location":"tutorial/","page":"Basic tutorial","title":"Basic tutorial","text":"This page was generated using Literate.jl.","category":"page"},{"location":"optim/#Optimization","page":"Optimization","title":"Optimization","text":"","category":"section"},{"location":"optim/","page":"Optimization","title":"Optimization","text":"info: Work in progress\nCome back later!","category":"page"},{"location":"optim/","page":"Optimization","title":"Optimization","text":"using AbstractTrees, InferOpt, InteractiveUtils\nAbstractTrees.children(x::Type) = subtypes(x)\nprint_tree(InferOpt.AbstractOptimizationLayer)","category":"page"}]
}
