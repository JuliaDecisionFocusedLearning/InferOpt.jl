<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Algorithms &amp; API · InferOpt.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://axelparmentier.github.io/InferOpt.jl/algorithms/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">InferOpt.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../math/">Mathematical background</a></li><li><a class="tocitem" href="../implementation/">Implementation</a></li><li class="is-active"><a class="tocitem" href>Algorithms &amp; API</a><ul class="internal"><li><a class="tocitem" href="#Index"><span>Index</span></a></li><li><a class="tocitem" href="#Interpolation"><span>Interpolation</span></a></li><li><a class="tocitem" href="#Smart-&quot;Predict,-then-Optimize&quot;"><span>Smart &quot;Predict, then Optimize&quot;</span></a></li><li><a class="tocitem" href="#Structured-Support-Vector-Machines"><span>Structured Support Vector Machines</span></a></li><li><a class="tocitem" href="#Regularized-optimizers"><span>Regularized optimizers</span></a></li><li><a class="tocitem" href="#Perturbed-optimizers"><span>Perturbed optimizers</span></a></li><li><a class="tocitem" href="#Fenchel-Young-losses"><span>Fenchel-Young losses</span></a></li><li><a class="tocitem" href="#Implicit-differentiation"><span>Implicit differentiation</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Algorithms &amp; API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Algorithms &amp; API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/axelparmentier/InferOpt.jl/blob/main/docs/src/algorithms.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#InferOpt.AbstractPerturbed-Tuple{AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}"><code>InferOpt.AbstractPerturbed</code></a></li><li><a href="#InferOpt.AbstractPerturbed"><code>InferOpt.AbstractPerturbed</code></a></li><li><a href="#InferOpt.FenchelYoungLoss"><code>InferOpt.FenchelYoungLoss</code></a></li><li><a href="#InferOpt.Interpolation"><code>InferOpt.Interpolation</code></a></li><li><a href="#InferOpt.IsBaseLoss"><code>InferOpt.IsBaseLoss</code></a></li><li><a href="#InferOpt.IsRegularized"><code>InferOpt.IsRegularized</code></a></li><li><a href="#InferOpt.PerturbedAdditive"><code>InferOpt.PerturbedAdditive</code></a></li><li><a href="#InferOpt.PerturbedComposition"><code>InferOpt.PerturbedComposition</code></a></li><li><a href="#InferOpt.PerturbedMultiplicative"><code>InferOpt.PerturbedMultiplicative</code></a></li><li><a href="#InferOpt.SPOPlusLoss"><code>InferOpt.SPOPlusLoss</code></a></li><li><a href="#InferOpt.StructuredSVMLoss"><code>InferOpt.StructuredSVMLoss</code></a></li><li><a href="#InferOpt.ZeroOneBaseLoss"><code>InferOpt.ZeroOneBaseLoss</code></a></li><li><a href="#InferOpt.compute_maximizer-Union{Tuple{L}, Tuple{Type{IsBaseLoss{L}}, L, Any, Any, Any}} where L"><code>InferOpt.compute_maximizer</code></a></li><li><a href="#InferOpt.compute_y_and_F-Tuple{AbstractPerturbed, AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}"><code>InferOpt.compute_y_and_F</code></a></li><li><a href="#InferOpt.half_square_norm-Tuple{AbstractArray{&lt;:Real}}"><code>InferOpt.half_square_norm</code></a></li><li><a href="#InferOpt.isproba-Tuple{Real}"><code>InferOpt.isproba</code></a></li><li><a href="#InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.isprobadist</code></a></li><li><a href="#InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.one_hot_argmax</code></a></li><li><a href="#InferOpt.positive_part-Tuple{Any}"><code>InferOpt.positive_part</code></a></li><li><a href="#InferOpt.ranking-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.ranking</code></a></li><li><a href="#InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.shannon_entropy</code></a></li><li><a href="#InferOpt.simplex_projection_and_support-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.simplex_projection_and_support</code></a></li><li><a href="#InferOpt.soft_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.soft_argmax</code></a></li><li><a href="#InferOpt.sparse_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.sparse_argmax</code></a></li></ul><h2 id="Interpolation"><a class="docs-heading-anchor" href="#Interpolation">Interpolation</a><a id="Interpolation-1"></a><a class="docs-heading-anchor-permalink" href="#Interpolation" title="Permalink"></a></h2><div class="admonition is-success"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/1912.02175">Differentiation of Blackbox Combinatorial Solvers</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.Interpolation" href="#InferOpt.Interpolation"><code>InferOpt.Interpolation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Interpolation{F}</code></pre><p>Piecewise-linear interpolation of a black-box optimizer.</p><p><strong>Fields</strong></p><ul><li><code>maximizer::F</code>: underlying argmax function</li><li><code>λ::Float64</code>: smoothing parameter (smaller = more faithful approximation, larger = more informative gradients)</li></ul><p>Reference: <a href="https://arxiv.org/abs/1912.02175">https://arxiv.org/abs/1912.02175</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/interpolation/interpolation.jl#L1-L11">source</a></section></article><h2 id="Smart-&quot;Predict,-then-Optimize&quot;"><a class="docs-heading-anchor" href="#Smart-&quot;Predict,-then-Optimize&quot;">Smart &quot;Predict, then Optimize&quot;</a><a id="Smart-&quot;Predict,-then-Optimize&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#Smart-&quot;Predict,-then-Optimize&quot;" title="Permalink"></a></h2><div class="admonition is-success"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/1710.08005">Smart &quot;Predict, then Optimize&quot;</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.SPOPlusLoss" href="#InferOpt.SPOPlusLoss"><code>InferOpt.SPOPlusLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SPOPlusLoss{F}</code></pre><p>Convex surrogate of the Smart &quot;Predict-then-Optimize&quot; loss.</p><p><strong>Fields</strong></p><ul><li><code>maximizer::F</code>: linear maximizer function of the form <code>θ ⟼ ŷ(θ) = argmax θᵀy</code></li><li><code>α::Float64</code>: convexification parameter</li></ul><p>Reference: <a href="https://arxiv.org/abs/1710.08005">https://arxiv.org/abs/1710.08005</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/spo/spoplus_loss.jl#L1-L11">source</a></section></article><h2 id="Structured-Support-Vector-Machines"><a class="docs-heading-anchor" href="#Structured-Support-Vector-Machines">Structured Support Vector Machines</a><a id="Structured-Support-Vector-Machines-1"></a><a class="docs-heading-anchor-permalink" href="#Structured-Support-Vector-Machines" title="Permalink"></a></h2><div class="admonition is-success"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://pub.ist.ac.at/~chl/papers/nowozin-fnt2011.pdf">Structured learning and prediction in computer vision</a>, Chapter 6</p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.IsBaseLoss" href="#InferOpt.IsBaseLoss"><code>InferOpt.IsBaseLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">IsBaseLoss{L}</code></pre><p>Trait-based interface for loss functions <code>δ(y, y_true)</code>, which are the base of the more complex <code>StructuredSVMLoss</code>.</p><p>For <code>δ::L</code> to comply with this interface, the following methods must exist:</p><ul><li><code>(δ)(y, y_true)</code></li><li><a href="#InferOpt.compute_maximizer-Union{Tuple{L}, Tuple{Type{IsBaseLoss{L}}, L, Any, Any, Any}} where L"><code>compute_maximizer(δ, θ, α, y_true)</code></a></li></ul><p><strong>Available implementations</strong></p><ul><li><a href="#InferOpt.ZeroOneBaseLoss"><code>ZeroOneBaseLoss</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/ssvm/isbaseloss.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_maximizer-Union{Tuple{L}, Tuple{Type{IsBaseLoss{L}}, L, Any, Any, Any}} where L" href="#InferOpt.compute_maximizer-Union{Tuple{L}, Tuple{Type{IsBaseLoss{L}}, L, Any, Any, Any}} where L"><code>InferOpt.compute_maximizer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compute_maximizer(δ, θ, α, y_true)</code></pre><p>Compute <code>argmax_y {δ(y, y_true) + α θᵀ(y - y_true)}</code> to deduce the gradient of a <code>StructuredSVMLoss</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/ssvm/isbaseloss.jl#L15-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.StructuredSVMLoss" href="#InferOpt.StructuredSVMLoss"><code>InferOpt.StructuredSVMLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StructuredSVMLoss{L}</code></pre><p>Loss associated with the Structured Support Vector Machine.</p><p><code>ℓ(θ, y_true) = max_y {δ(y, y_true) + α θᵀ(y - y_true)}</code></p><p><strong>Fields</strong></p><ul><li><code>base_loss::L</code>:  of the <code>IsBaseLoss</code> trait</li><li><code>α::Float64</code></li></ul><p>Reference: <a href="http://www.nowozin.net/sebastian/papers/nowozin2011structured-tutorial.pdf">http://www.nowozin.net/sebastian/papers/nowozin2011structured-tutorial.pdf</a> (Chapter 6)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/ssvm/ssvm_loss.jl#L1-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ZeroOneBaseLoss" href="#InferOpt.ZeroOneBaseLoss"><code>InferOpt.ZeroOneBaseLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZeroOneBaseLoss</code></pre><p>0-1 loss for multiclass classification: <code>δ(y, y_true) = 0</code> if <code>y = y_true</code>, and <code>1</code> otherwise.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/ssvm/zeroone_baseloss.jl#L1-L5">source</a></section></article><h2 id="Regularized-optimizers"><a class="docs-heading-anchor" href="#Regularized-optimizers">Regularized optimizers</a><a id="Regularized-optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Regularized-optimizers" title="Permalink"></a></h2><div class="admonition is-success"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/1901.02324">Learning with Fenchel-Young Losses</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.IsRegularized" href="#InferOpt.IsRegularized"><code>InferOpt.IsRegularized</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">IsRegularized{P}</code></pre><p>Trait-based interface for regularized prediction functions <code>ŷ(θ) = argmax {θᵀy - Ω(y)}</code>.</p><p>For <code>predictor::P</code> to comply with this interface, the following methods must exist:</p><ul><li><code>(predictor)(θ)</code></li><li><code>compute_regularization(predictor, y)</code></li></ul><p><strong>Available implementations</strong></p><ul><li><a href="#InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>one_hot_argmax</code></a></li><li><a href="#InferOpt.soft_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>soft_argmax</code></a></li><li><a href="#InferOpt.sparse_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>sparse_argmax</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/isregularized.jl#L1-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.soft_argmax-Tuple{AbstractVector{&lt;:Real}}" href="#InferOpt.soft_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.soft_argmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">soft_argmax(z)</code></pre><p>Soft argmax activation function <code>s(z) = (e^zᵢ / ∑ e^zⱼ)ᵢ</code>.</p><p>Corresponds to regularized prediction on the probability simplex with entropic penalty.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/soft_argmax.jl#L1-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.simplex_projection_and_support-Tuple{AbstractVector{&lt;:Real}}" href="#InferOpt.simplex_projection_and_support-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.simplex_projection_and_support</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">simplex_projection_and_support(z)</code></pre><p>Compute the Euclidean projection <code>p</code> of <code>z</code> on the probability simplex (also called <a href="#InferOpt.sparse_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>sparse_argmax</code></a>), and the indicators <code>s</code> of its support.</p><p>Reference: <a href="https://arxiv.org/abs/1602.02068">https://arxiv.org/abs/1602.02068</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/sparse_argmax.jl#L21-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.sparse_argmax-Tuple{AbstractVector{&lt;:Real}}" href="#InferOpt.sparse_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.sparse_argmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sparse_argmax(z)</code></pre><p>Compute the Euclidean projection of the vector <code>z</code> onto the probability simplex.</p><p>Corresponds to regularized prediction on the probability simplex with square norm penalty.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/sparse_argmax.jl#L1-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.half_square_norm-Tuple{AbstractArray{&lt;:Real}}" href="#InferOpt.half_square_norm-Tuple{AbstractArray{&lt;:Real}}"><code>InferOpt.half_square_norm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">half_square_norm(x)</code></pre><p>Compute the squared Euclidean norm of <code>x</code> and divide it by 2.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/regularized_utils.jl#L22-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.isproba-Tuple{Real}" href="#InferOpt.isproba-Tuple{Real}"><code>InferOpt.isproba</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">isproba(x)</code></pre><p>Check whether <code>x ∈ [0,1]</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/regularized_utils.jl#L8-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real" href="#InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.isprobadist</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">isprobadist(p)</code></pre><p>Check whether the elements of <code>p</code> are nonnegative and sum to 1.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/regularized_utils.jl#L15-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real" href="#InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.one_hot_argmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">one_hot_argmax(z)</code></pre><p>One-hot encoding of the argmax function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/regularized_utils.jl#L48-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.positive_part-Tuple{Any}" href="#InferOpt.positive_part-Tuple{Any}"><code>InferOpt.positive_part</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">positive_part(x)</code></pre><p>Compute <code>max(x,0)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/regularized_utils.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ranking-Tuple{AbstractVector{&lt;:Real}}" href="#InferOpt.ranking-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.ranking</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ranking(θ[; rev])</code></pre><p>Compute the vector <code>r</code> such that <code>rᵢ</code> is the rank of <code>θᵢ</code> in <code>θ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/regularized_utils.jl#L59-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real" href="#InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.shannon_entropy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">shannon_entropy(p)</code></pre><p>Compute the Shannon entropy of a probability distribution: <code>H(p) = -∑ pᵢlog(pᵢ)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/regularized/regularized_utils.jl#L31-L35">source</a></section></article><h2 id="Perturbed-optimizers"><a class="docs-heading-anchor" href="#Perturbed-optimizers">Perturbed optimizers</a><a id="Perturbed-optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Perturbed-optimizers" title="Permalink"></a></h2><div class="admonition is-success"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/2002.08676">Learning with Differentiable Perturbed Optimizers</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.AbstractPerturbed" href="#InferOpt.AbstractPerturbed"><code>InferOpt.AbstractPerturbed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractPerturbed{F}</code></pre><p>Differentiable perturbation of a black-box optimizer.</p><p><strong>Available subtypes</strong></p><ul><li><a href="#InferOpt.PerturbedAdditive"><code>PerturbedAdditive{F}</code></a></li><li><a href="#InferOpt.PerturbedMultiplicative"><code>PerturbedMultiplicative{F}</code></a></li></ul><p><strong>Required fields</strong></p><ul><li><code>maximizer::F</code>: underlying argmax function</li><li><code>ε::Float64</code>: noise scaling parameter</li><li><code>rng::AbstractRNG</code>: random number generator</li><li><code>seed::Union{Nothing,Int}</code>: random seed</li><li><code>nb_samples::Int</code>: number of random samples for Monte-Carlo computations</li></ul><p><strong>Required methods</strong></p><ul><li><code>(perturbed)(θ, Z; kwargs...)</code></li><li><a href="#InferOpt.compute_y_and_F-Tuple{AbstractPerturbed, AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}"><code>compute_y_and_F(perturbed, θ, Z; kwargs...)</code></a></li></ul><p><strong>Optional methods</strong></p><ul><li><code>rrule(perturbed, θ; kwargs...)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/perturbed/abstract_perturbed.jl#L1-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.AbstractPerturbed-Tuple{AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}" href="#InferOpt.AbstractPerturbed-Tuple{AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}"><code>InferOpt.AbstractPerturbed</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(perturbed)(θ, Z; kwargs...)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/perturbed/abstract_perturbed.jl#L33-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_y_and_F-Tuple{AbstractPerturbed, AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}" href="#InferOpt.compute_y_and_F-Tuple{AbstractPerturbed, AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}"><code>InferOpt.compute_y_and_F</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compute_y_and_F(perturbed, θ, Z; kwargs...)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/perturbed/abstract_perturbed.jl#L48-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedAdditive" href="#InferOpt.PerturbedAdditive"><code>InferOpt.PerturbedAdditive</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PerturbedAdditive{F}</code></pre><p>Differentiable normal perturbation of a black-box optimizer: the input undergoes <code>θ -&gt; θ + εZ</code> where <code>Z ∼ N(0, I)</code>.</p><p>See also: <a href="#InferOpt.AbstractPerturbed"><code>AbstractPerturbed{F}</code></a>.</p><p>Reference: <a href="https://arxiv.org/abs/2002.08676">https://arxiv.org/abs/2002.08676</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/perturbed/additive.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedComposition" href="#InferOpt.PerturbedComposition"><code>InferOpt.PerturbedComposition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PerturbedComposition{F,P&lt;:AbstractPerturbed{F},G}</code></pre><p>Composition of a differentiable perturbed black-box optimizer with an arbitrary function.</p><p>Suitable for direct regret minimization (learning by experience) when said function is a cost.</p><p><strong>Fields</strong></p><ul><li><code>perturbed::P</code>: underlying <a href="#InferOpt.AbstractPerturbed"><code>AbstractPerturbed{F}</code></a> wrapper</li><li><code>g::G</code>: function taking an array <code>y</code> and some <code>kwargs</code> as inputs</li></ul><p>The method <a href="#ChainRulesCore.rrule-Tuple{PerturbedComposition, AbstractArray{&lt;:Real}}"><code>rrule(perturbed_composition, θ; kwargs...)</code></a> must be implemented individually for each specific type <code>P</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/perturbed/composition.jl#L1-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.:∘-Tuple{Any, AbstractPerturbed}" href="#Base.:∘-Tuple{Any, AbstractPerturbed}"><code>Base.:∘</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">∘(g, perturbed)</code></pre><p>Create a <code>PerturbedComposition</code> object from <code>perturbed</code> and <code>g</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/perturbed/composition.jl#L24-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ChainRulesCore.rrule-Tuple{PerturbedComposition, AbstractArray{&lt;:Real}}" href="#ChainRulesCore.rrule-Tuple{PerturbedComposition, AbstractArray{&lt;:Real}}"><code>ChainRulesCore.rrule</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rrule(perturbed_composition, θ; kwargs...)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/perturbed/composition.jl#L40-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedMultiplicative" href="#InferOpt.PerturbedMultiplicative"><code>InferOpt.PerturbedMultiplicative</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PerturbedMultiplicative{F}</code></pre><p>Differentiable log-normal perturbation of a black-box optimizer: the input undergoes <code>θ -&gt; θ ⊙ exp[εZ - ε²/2]</code> where <code>Z ∼ N(0, I)</code>.</p><p>See also: <a href="#InferOpt.AbstractPerturbed"><code>AbstractPerturbed{F}</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/perturbed/multiplicative.jl#L1-L7">source</a></section></article><h2 id="Fenchel-Young-losses"><a class="docs-heading-anchor" href="#Fenchel-Young-losses">Fenchel-Young losses</a><a id="Fenchel-Young-losses-1"></a><a class="docs-heading-anchor-permalink" href="#Fenchel-Young-losses" title="Permalink"></a></h2><div class="admonition is-success"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/1901.02324">Learning with Fenchel-Young Losses</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.FenchelYoungLoss" href="#InferOpt.FenchelYoungLoss"><code>InferOpt.FenchelYoungLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FenchelYoungLoss{P}</code></pre><p>Fenchel-Young loss associated with a given regularized prediction function.</p><p><strong>Fields</strong></p><ul><li><code>predictor::P</code>: prediction function of the form <code>ŷ(θ) = argmax {θᵀy - Ω(y)}</code></li></ul><p>Reference: <a href="https://arxiv.org/abs/1901.02324">https://arxiv.org/abs/1901.02324</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/d80b510bb72fcf3afa96fc680f2c5c23f8c34dd6/src/fenchel_young/fenchel_young.jl#L1-L10">source</a></section></article><h2 id="Implicit-differentiation"><a class="docs-heading-anchor" href="#Implicit-differentiation">Implicit differentiation</a><a id="Implicit-differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Implicit-differentiation" title="Permalink"></a></h2><div class="admonition is-success"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="http://arxiv.org/abs/2105.15183">Efficient and Modular Implicit Differentiation</a></p></div></div><div class="admonition is-info"><header class="admonition-header">Stay tuned!</header><div class="admonition-body"><p>This will soon be implemented thanks to the recent package <a href="https://github.com/gdalle/ImplicitDifferentiation.jl">ImplicitDifferentiation.jl</a>.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../implementation/">« Implementation</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Saturday 25 June 2022 07:00">Saturday 25 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
