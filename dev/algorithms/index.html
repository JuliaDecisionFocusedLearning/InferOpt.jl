<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Algorithms &amp; API · InferOpt.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://axelparmentier.github.io/InferOpt.jl/algorithms/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">InferOpt.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorial/">Basic tutorial</a></li><li><a class="tocitem" href="../advanced_tutorials/">Advanced applications</a></li></ul></li><li><a class="tocitem" href="../background/">Background</a></li><li class="is-active"><a class="tocitem" href>Algorithms &amp; API</a><ul class="internal"><li><a class="tocitem" href="#Probability-distributions"><span>Probability distributions</span></a></li><li><a class="tocitem" href="#Interpolation"><span>Interpolation</span></a></li><li><a class="tocitem" href="#Smart-&quot;Predict,-then-Optimize&quot;"><span>Smart &quot;Predict, then Optimize&quot;</span></a></li><li><a class="tocitem" href="#Structured-Support-Vector-Machines"><span>Structured Support Vector Machines</span></a></li><li><a class="tocitem" href="#Frank-Wolfe"><span>Frank-Wolfe</span></a></li><li><a class="tocitem" href="#Regularized-optimizers"><span>Regularized optimizers</span></a></li><li><a class="tocitem" href="#Perturbed-optimizers"><span>Perturbed optimizers</span></a></li><li><a class="tocitem" href="#Fenchel-Young-losses"><span>Fenchel-Young losses</span></a></li><li><a class="tocitem" href="#Generalized-imitation-losses"><span>Generalized imitation losses</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Algorithms &amp; API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Algorithms &amp; API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/axelparmentier/InferOpt.jl/blob/main/docs/src/algorithms.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><h2 id="Probability-distributions"><a class="docs-heading-anchor" href="#Probability-distributions">Probability distributions</a><a id="Probability-distributions-1"></a><a class="docs-heading-anchor-permalink" href="#Probability-distributions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="InferOpt.FixedAtomsProbabilityDistribution" href="#InferOpt.FixedAtomsProbabilityDistribution"><code>InferOpt.FixedAtomsProbabilityDistribution</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FixedAtomsProbabilityDistribution{A,W}</code></pre><p>Encodes a probability distribution with finite support and fixed atoms.</p><p>See <a href="#InferOpt.compute_expectation"><code>compute_expectation</code></a> to understand the name of this struct.</p><p><strong>Fields</strong></p><ul><li><code>atoms::Vector{A}</code>: elements of the support</li><li><code>weights::Vector{W}</code>: probability values for each atom (must sum to 1)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/utils/probability_distribution.jl#L1-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.FixedAtomsProbabilityDistribution-Tuple{FrankWolfe.ActiveSet}" href="#InferOpt.FixedAtomsProbabilityDistribution-Tuple{FrankWolfe.ActiveSet}"><code>InferOpt.FixedAtomsProbabilityDistribution</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">FixedAtomsProbabilityDistribution(s::FrankWolfe.ActiveSet)</code></pre><p>Construct a distribution from the active set generated by a Frank-Wolfe algorithm.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/utils/probability_distribution.jl#L25-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.rand-Tuple{Random.AbstractRNG, FixedAtomsProbabilityDistribution}" href="#Base.rand-Tuple{Random.AbstractRNG, FixedAtomsProbabilityDistribution}"><code>Base.rand</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rand([rng,] probadist)</code></pre><p>Sample from the atoms of <code>probadist</code> according to their weights.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/utils/probability_distribution.jl#L36-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.apply_on_atoms-Tuple{Any, FixedAtomsProbabilityDistribution}" href="#InferOpt.apply_on_atoms-Tuple{Any, FixedAtomsProbabilityDistribution}"><code>InferOpt.apply_on_atoms</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">apply_on_atoms(post_processing, probadist)</code></pre><p>Create a new distribution by applying the function <code>post_processing</code> to each atom of <code>probadist</code> (the weights remain the same).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/utils/probability_distribution.jl#L75-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compress_distribution!-Union{Tuple{FixedAtomsProbabilityDistribution{A, W}}, Tuple{W}, Tuple{A}} where {A, W}" href="#InferOpt.compress_distribution!-Union{Tuple{FixedAtomsProbabilityDistribution{A, W}}, Tuple{W}, Tuple{A}} where {A, W}"><code>InferOpt.compress_distribution!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compress_distribution!(probadist[; atol])</code></pre><p>Remove duplicated atoms in <code>probadist</code> (up to a tolerance on equality).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/utils/probability_distribution.jl#L48-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_expectation" href="#InferOpt.compute_expectation"><code>InferOpt.compute_expectation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">compute_expectation(probadist[, post_processing=identity])</code></pre><p>Compute the expectation of <code>post_processing(X)</code> where <code>X</code> is a random variable distributed according to <code>probadist</code>.</p><p>This operation is made differentiable thanks to a custom reverse rule, even when <code>post_processing</code> itself is not a differentiable function.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Derivatives are computed with respect to <code>probadist.weights</code> only, assuming that <code>probadist.atoms</code> doesn&#39;t change (hence the name <a href="#InferOpt.FixedAtomsProbabilityDistribution"><code>FixedAtomsProbabilityDistribution</code></a>).</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/utils/probability_distribution.jl#L88-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_probability_distribution" href="#InferOpt.compute_probability_distribution"><code>InferOpt.compute_probability_distribution</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">compute_probability_distribution(layer, θ)</code></pre><p>Apply a probabilistic layer (regularized or perturbed) to an objective direction <code>θ</code> in order to generate a <a href="#InferOpt.FixedAtomsProbabilityDistribution"><code>FixedAtomsProbabilityDistribution</code></a> on the vertices of a polytope.</p><p>The following layer types are supported:</p><ul><li><a href="#InferOpt.AbstractPerturbed"><code>AbstractPerturbed</code></a></li><li><a href="#InferOpt.RegularizedGeneric"><code>RegularizedGeneric</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/utils/probability_distribution.jl#L123-L131">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.Pushforward" href="#InferOpt.Pushforward"><code>InferOpt.Pushforward</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Pushforward{L,G}</code></pre><p>Differentiable pushforward of a probabilistic <code>layer</code> with an arbitrary function <code>post_processing</code>.</p><p><code>Pushforward</code> can be used for direct regret minimization (aka learning by experience) when the post-processing returns a cost.</p><p><strong>Fields</strong></p><ul><li><code>layer::L</code>: anything that implements <code>compute_probability_distribution(layer, θ; kwargs...)</code></li><li><code>post_processing::P</code>: callable</li></ul><p>See also: <a href="#InferOpt.FixedAtomsProbabilityDistribution"><code>FixedAtomsProbabilityDistribution</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/utils/pushforward.jl#L1-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.Pushforward-Tuple{AbstractArray{&lt;:Real}}" href="#InferOpt.Pushforward-Tuple{AbstractArray{&lt;:Real}}"><code>InferOpt.Pushforward</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(pushforward::Pushforward)(θ)</code></pre><p>Output the expectation of <code>pushforward.post_processing(X)</code>, where <code>X</code> follows the distribution defined by <code>pushforward.layer</code> applied to <code>θ</code>.</p><p>Unlike <a href="#InferOpt.compute_probability_distribution"><code>compute_probability_distribution(pushforward, θ)</code></a>, this function is differentiable, even if <code>pushforward.post_processing</code> isn&#39;t.</p><p>See also: <a href="#InferOpt.compute_expectation"><code>compute_expectation</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/utils/pushforward.jl#L40-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_probability_distribution-Tuple{Pushforward, Any}" href="#InferOpt.compute_probability_distribution-Tuple{Pushforward, Any}"><code>InferOpt.compute_probability_distribution</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compute_probability_distribution(pushforward, θ)</code></pre><p>Output the distribution of <code>pushforward.post_processing(X)</code>, where <code>X</code> follows the distribution defined by <code>pushforward.layer</code> applied to <code>θ</code>.</p><p>This function is not differentiable if <code>pushforward.post_processing</code> isn&#39;t.</p><p>See also: <a href="#InferOpt.apply_on_atoms-Tuple{Any, FixedAtomsProbabilityDistribution}"><code>apply_on_atoms</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/utils/pushforward.jl#L24-L32">source</a></section></article><h2 id="Interpolation"><a class="docs-heading-anchor" href="#Interpolation">Interpolation</a><a id="Interpolation-1"></a><a class="docs-heading-anchor-permalink" href="#Interpolation" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/1912.02175">Differentiation of Blackbox Combinatorial Solvers</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.Interpolation" href="#InferOpt.Interpolation"><code>InferOpt.Interpolation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Interpolation{F}</code></pre><p>Piecewise-linear interpolation of a black-box optimizer.</p><p><strong>Fields</strong></p><ul><li><code>maximizer::F</code>: underlying argmax function</li><li><code>λ::Float64</code>: smoothing parameter (smaller = more faithful approximation, larger = more informative gradients)</li></ul><p>Reference: <a href="https://arxiv.org/abs/1912.02175">https://arxiv.org/abs/1912.02175</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/interpolation/interpolation.jl#L1-L11">source</a></section></article><h2 id="Smart-&quot;Predict,-then-Optimize&quot;"><a class="docs-heading-anchor" href="#Smart-&quot;Predict,-then-Optimize&quot;">Smart &quot;Predict, then Optimize&quot;</a><a id="Smart-&quot;Predict,-then-Optimize&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#Smart-&quot;Predict,-then-Optimize&quot;" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/1710.08005">Smart &quot;Predict, then Optimize&quot;</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.SPOPlusLoss" href="#InferOpt.SPOPlusLoss"><code>InferOpt.SPOPlusLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SPOPlusLoss{F}</code></pre><p>Convex surrogate of the Smart &quot;Predict-then-Optimize&quot; loss.</p><p><strong>Fields</strong></p><ul><li><code>maximizer::F</code>: linear maximizer function of the form <code>θ ⟼ ŷ(θ) = argmax θᵀy</code></li><li><code>α::Float64</code>: convexification parameter</li></ul><p>Reference: <a href="https://arxiv.org/abs/1710.08005">https://arxiv.org/abs/1710.08005</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/spo/spoplus_loss.jl#L1-L11">source</a></section></article><h2 id="Structured-Support-Vector-Machines"><a class="docs-heading-anchor" href="#Structured-Support-Vector-Machines">Structured Support Vector Machines</a><a id="Structured-Support-Vector-Machines-1"></a><a class="docs-heading-anchor-permalink" href="#Structured-Support-Vector-Machines" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://pub.ist.ac.at/~chl/papers/nowozin-fnt2011.pdf">Structured learning and prediction in computer vision</a>, Chapter 6</p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.IsBaseLoss" href="#InferOpt.IsBaseLoss"><code>InferOpt.IsBaseLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">IsBaseLoss{L}</code></pre><p>Trait-based interface for loss functions <code>δ(y, y_true)</code>, which are the base of the more complex <code>StructuredSVMLoss</code>.</p><p>For <code>δ::L</code> to comply with this interface, the following methods must exist:</p><ul><li><code>(δ)(y, y_true)</code></li><li><a href="#InferOpt.compute_maximizer-Union{Tuple{L}, Tuple{Type{IsBaseLoss{L}}, L, Any, Any, Any}} where L"><code>compute_maximizer(δ, θ, α, y_true)</code></a></li></ul><p><strong>Available implementations</strong></p><ul><li><a href="#InferOpt.ZeroOneBaseLoss"><code>ZeroOneBaseLoss</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/ssvm/isbaseloss.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_maximizer-Union{Tuple{L}, Tuple{Type{IsBaseLoss{L}}, L, Any, Any, Any}} where L" href="#InferOpt.compute_maximizer-Union{Tuple{L}, Tuple{Type{IsBaseLoss{L}}, L, Any, Any, Any}} where L"><code>InferOpt.compute_maximizer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compute_maximizer(δ, θ, α, y_true)</code></pre><p>Compute <code>argmax_y {δ(y, y_true) + α θᵀ(y - y_true)}</code> to deduce the gradient of a <code>StructuredSVMLoss</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/ssvm/isbaseloss.jl#L15-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.StructuredSVMLoss" href="#InferOpt.StructuredSVMLoss"><code>InferOpt.StructuredSVMLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StructuredSVMLoss{L}</code></pre><p>Loss associated with the Structured Support Vector Machine.</p><p><code>SSVM(θ, y_true) = max_y {base_loss(y, y_true) + α θᵀ(y - y_true)}</code></p><p><strong>Fields</strong></p><ul><li><code>base_loss::L</code>:  of the <code>IsBaseLoss</code> trait</li><li><code>α::Float64</code></li></ul><p>Reference: <a href="http://www.nowozin.net/sebastian/papers/nowozin2011structured-tutorial.pdf">http://www.nowozin.net/sebastian/papers/nowozin2011structured-tutorial.pdf</a> (Chapter 6)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/ssvm/ssvm_loss.jl#L1-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ZeroOneBaseLoss" href="#InferOpt.ZeroOneBaseLoss"><code>InferOpt.ZeroOneBaseLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZeroOneBaseLoss</code></pre><p>0-1 loss for multiclass classification: <code>δ(y, y_true) = 0</code> if <code>y = y_true</code>, and <code>1</code> otherwise.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/ssvm/zeroone_baseloss.jl#L1-L5">source</a></section></article><h2 id="Frank-Wolfe"><a class="docs-heading-anchor" href="#Frank-Wolfe">Frank-Wolfe</a><a id="Frank-Wolfe-1"></a><a class="docs-heading-anchor-permalink" href="#Frank-Wolfe" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="http://arxiv.org/abs/2105.15183">Efficient and Modular Implicit Differentiation</a></p></div></div><div class="admonition is-info"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/2104.06675">FrankWolfe.jl: a high-performance and flexible toolbox for Frank-Wolfe algorithms and Conditional Gradients</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.DEFAULT_FRANK_WOLFE_KWARGS" href="#InferOpt.DEFAULT_FRANK_WOLFE_KWARGS"><code>InferOpt.DEFAULT_FRANK_WOLFE_KWARGS</code></a> — <span class="docstring-category">Constant</span></header><section><div><pre><code class="language-julia hljs">DEFAULT_FRANK_WOLFE_KWARGS</code></pre><p>Default configuration for the Frank-Wolfe wrapper.</p><p><strong>Parameters</strong></p><ul><li><code>away_steps=true</code>: activate away steps to avoid zig-zagging</li><li><code>epsilon=1e-4</code>: precision</li><li><code>lazy=true</code>: caching strategy</li><li><code>line_search=FrankWolfe.Agnostic()</code>: step size selection</li><li><code>max_iteration=10</code>: number of iterations</li><li><code>timeout=1.0</code>: maximum time in seconds</li><li><code>verbose=false</code>: console output</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/frank_wolfe/frank_wolfe_utils.jl#L1-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.LMOWrapper" href="#InferOpt.LMOWrapper"><code>InferOpt.LMOWrapper</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LMOWrapper{F,K}</code></pre><p>Wraps a linear maximizer as a <code>FrankWolfe.LinearMinimizationOracle</code>.</p><p><strong>Fields</strong></p><ul><li><code>maximizer::F</code>: black box linear maximizer</li><li><code>maximizer_kwargs::K</code>: keyword arguments passed to the maximizer whenever it is called</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/frank_wolfe/frank_wolfe_utils.jl#L27-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="FrankWolfe.compute_extreme_point-Tuple{LMOWrapper, Any}" href="#FrankWolfe.compute_extreme_point-Tuple{LMOWrapper, Any}"><code>FrankWolfe.compute_extreme_point</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">FrankWolfe.compute_extreme_point(lmo_wrapper::LMOWrapper, direction)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/frank_wolfe/frank_wolfe_utils.jl#L43-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.DifferentiableFrankWolfe" href="#InferOpt.DifferentiableFrankWolfe"><code>InferOpt.DifferentiableFrankWolfe</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferentiableFrankWolfe{F,G,M,S}</code></pre><p>Parameterized version of the Frank-Wolfe algorithm <code>θ -&gt; argmin_{x ∈ C} f(x, θ)</code>, which can be differentiated implicitly wrt <code>θ</code>.</p><p><strong>Fields</strong></p><ul><li><code>f::F</code>: function <code>f(x, θ)</code> to minimize wrt <code>x</code></li><li><code>f_grad1::G</code>: gradient <code>∇ₓf(x, θ)</code> of <code>f</code> wrt <code>x</code></li><li><code>lmo::M</code>: linear minimization oracle <code>θ -&gt; argmin_{x ∈ C} θᵀx</code>, implicitly defines the polytope <code>C</code></li><li><code>linear_solver::S</code>: solver for linear systems of equations, used during implicit differentiation</li></ul><p><strong>Applicable methods</strong></p><ul><li><a href="#InferOpt.compute_probability_distribution"><code>compute_probability_distribution(dfw::DifferentiableFrankWolfe, θ, x0)</code></a></li><li><code>(dfw::DifferentiableFrankWolfe)(θ, x0)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/frank_wolfe/differentiable_frank_wolfe.jl#L3-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.DifferentiableFrankWolfe-Tuple{AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}" href="#InferOpt.DifferentiableFrankWolfe-Tuple{AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}"><code>InferOpt.DifferentiableFrankWolfe</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(dfw::DifferentiableFrankWolfe)(θ, x0[; fw_kwargs=(;)])</code></pre><p>Apply <code>compute_probability_distribution(dfw, θ, x0)</code> and return the expectation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/frank_wolfe/differentiable_frank_wolfe.jl#L68-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_probability_distribution-Tuple{DifferentiableFrankWolfe, AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}" href="#InferOpt.compute_probability_distribution-Tuple{DifferentiableFrankWolfe, AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}"><code>InferOpt.compute_probability_distribution</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compute_probability_distribution(dfw::DifferentiableFrankWolfe, θ, x0[; fw_kwargs=(;)])</code></pre><p>Compute the optimal active set by applying the away-step Frank-Wolfe algorithm with initial point <code>x0</code>, then turn it into a probability distribution.</p><p>The named tuple <code>fw_kwargs</code> is passed as keyword arguments to <code>FrankWolfe.away_frank_wolfe</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/frank_wolfe/differentiable_frank_wolfe.jl#L42-L48">source</a></section></article><h2 id="Regularized-optimizers"><a class="docs-heading-anchor" href="#Regularized-optimizers">Regularized optimizers</a><a id="Regularized-optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Regularized-optimizers" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/1901.02324">Learning with Fenchel-Young Losses</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.IsRegularized" href="#InferOpt.IsRegularized"><code>InferOpt.IsRegularized</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">IsRegularized{P}</code></pre><p>Trait-based interface for regularized prediction functions <code>ŷ(θ) = argmax {θᵀy - Ω(y)}</code>.</p><p>For <code>predictor::P</code> to comply with this interface, the following methods must exist:</p><ul><li><code>(predictor)(θ)</code></li><li><code>compute_regularization(predictor, y)</code></li></ul><p><strong>Available implementations</strong></p><ul><li><a href="#InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>one_hot_argmax</code></a></li><li><a href="#InferOpt.soft_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>soft_argmax</code></a></li><li><a href="#InferOpt.sparse_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>sparse_argmax</code></a></li><li><a href="#InferOpt.RegularizedGeneric"><code>RegularizedGeneric</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/isregularized.jl#L1-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_regularization" href="#InferOpt.compute_regularization"><code>InferOpt.compute_regularization</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">compute_regularization(predictor::P, y)</code></pre><p>Compute the convex regularization function <code>Ω(y)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/isregularized.jl#L18-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.RegularizedGeneric" href="#InferOpt.RegularizedGeneric"><code>InferOpt.RegularizedGeneric</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RegularizedGeneric{M,RF,RG,F,G,S}</code></pre><p>Differentiable regularized prediction function <code>ŷ(θ) = argmax_{y ∈ C} {θᵀy - Ω(y)}</code>.</p><p>Relies on the Frank-Wolfe algorithm to minimize a concave objective on a polytope.</p><p><strong>Fields</strong></p><ul><li><code>maximizer::M</code>: linear maximization oracle <code>θ -&gt; argmax_{x ∈ C} θᵀx</code>, implicitly defines the polytope <code>C</code></li><li><code>Ω::RF</code>: regularization function <code>Ω(y)</code></li><li><code>Ω_grad::RG</code>: gradient of the regularization function <code>∇Ω(y)</code></li><li><code>f::F</code>: objective function <code>f(x, θ) = Ω(y) - θᵀy</code> minimized by Frank-Wolfe (computed automatically)</li><li><code>f_grad1::G</code>: gradient of the objective function <code>∇ₓf(x, θ) = ∇Ω(y) - θ</code> with respect to <code>x</code> (computed automatically)</li><li><code>linear_solver::S</code>: solver for linear systems of equations, used during implicit differentiation</li></ul><p><strong>Applicable methods</strong></p><ul><li><a href="#InferOpt.compute_probability_distribution"><code>compute_probability_distribution(regularized::RegularizedGeneric, θ)</code></a></li><li><code>(regularized::RegularizedGeneric)(θ)</code></li></ul><p>See also: <a href="#InferOpt.DifferentiableFrankWolfe"><code>DifferentiableFrankWolfe</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_generic.jl#L1-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.RegularizedGeneric-Tuple{AbstractArray{&lt;:Real}}" href="#InferOpt.RegularizedGeneric-Tuple{AbstractArray{&lt;:Real}}"><code>InferOpt.RegularizedGeneric</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(regularized::RegularizedGeneric)(θ[; maximizer_kwargs=(;), fw_kwargs=(;)])</code></pre><p>Apply <code>compute_probability_distribution(regularized, θ)</code> and return the expectation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_generic.jl#L94-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.RegularizedGeneric-Tuple{Any}" href="#InferOpt.RegularizedGeneric-Tuple{Any}"><code>InferOpt.RegularizedGeneric</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">RegularizedGeneric(maximizer[; Ω, Ω_grad, linear_solver=gmres])</code></pre><p>Shorter constructor with defaults.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_generic.jl#L43-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_probability_distribution-Tuple{RegularizedGeneric, AbstractArray{&lt;:Real}}" href="#InferOpt.compute_probability_distribution-Tuple{RegularizedGeneric, AbstractArray{&lt;:Real}}"><code>InferOpt.compute_probability_distribution</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compute_probability_distribution(regularized::RegularizedGeneric, θ[; maximizer_kwargs=(;), fw_kwargs=(;)])</code></pre><p>Construct a <a href="#InferOpt.DifferentiableFrankWolfe"><code>DifferentiableFrankWolfe</code></a> struct and call <code>compute_probability_distribution</code> on it.</p><p>The named tuple <code>maximizer_kwargs</code> is passed as keyword arguments to the underlying maximizer, which is wrapped inside a <a href="#InferOpt.LMOWrapper"><code>LMOWrapper</code></a>. The named tuple <code>fw_kwargs</code> is passed as keyword arguments to <code>FrankWolfe.away_frank_wolfe</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_generic.jl#L71-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.half_square_norm-Tuple{AbstractArray{&lt;:Real}}" href="#InferOpt.half_square_norm-Tuple{AbstractArray{&lt;:Real}}"><code>InferOpt.half_square_norm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">half_square_norm(x)</code></pre><p>Compute the squared Euclidean norm of <code>x</code> and divide it by 2.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_utils.jl#L22-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.isproba-Tuple{Real}" href="#InferOpt.isproba-Tuple{Real}"><code>InferOpt.isproba</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">isproba(x)</code></pre><p>Check whether <code>x ∈ [0,1]</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_utils.jl#L8-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real" href="#InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.isprobadist</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">isprobadist(p)</code></pre><p>Check whether the elements of <code>p</code> are nonnegative and sum to 1.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_utils.jl#L15-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real" href="#InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.one_hot_argmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">one_hot_argmax(z)</code></pre><p>One-hot encoding of the argmax function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_utils.jl#L48-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.positive_part-Tuple{Any}" href="#InferOpt.positive_part-Tuple{Any}"><code>InferOpt.positive_part</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">positive_part(x)</code></pre><p>Compute <code>max(x,0)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_utils.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ranking-Tuple{AbstractVector{&lt;:Real}}" href="#InferOpt.ranking-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.ranking</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ranking(θ[; rev])</code></pre><p>Compute the vector <code>r</code> such that <code>rᵢ</code> is the rank of <code>θᵢ</code> in <code>θ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_utils.jl#L59-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real" href="#InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.shannon_entropy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">shannon_entropy(p)</code></pre><p>Compute the Shannon entropy of a probability distribution: <code>H(p) = -∑ pᵢlog(pᵢ)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/regularized_utils.jl#L31-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.soft_argmax-Tuple{AbstractVector{&lt;:Real}}" href="#InferOpt.soft_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.soft_argmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">soft_argmax(z)</code></pre><p>Soft argmax activation function <code>s(z) = (e^zᵢ / ∑ e^zⱼ)ᵢ</code>.</p><p>Corresponds to regularized prediction on the probability simplex with entropic penalty.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/soft_argmax.jl#L1-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.simplex_projection_and_support-Tuple{AbstractVector{&lt;:Real}}" href="#InferOpt.simplex_projection_and_support-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.simplex_projection_and_support</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">simplex_projection_and_support(z)</code></pre><p>Compute the Euclidean projection <code>p</code> of <code>z</code> on the probability simplex (also called <a href="#InferOpt.sparse_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>sparse_argmax</code></a>), and the indicators <code>s</code> of its support.</p><p>Reference: <a href="https://arxiv.org/abs/1602.02068">https://arxiv.org/abs/1602.02068</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/sparse_argmax.jl#L21-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.sparse_argmax-Tuple{AbstractVector{&lt;:Real}}" href="#InferOpt.sparse_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.sparse_argmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sparse_argmax(z)</code></pre><p>Compute the Euclidean projection of the vector <code>z</code> onto the probability simplex.</p><p>Corresponds to regularized prediction on the probability simplex with square norm penalty.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/regularized/sparse_argmax.jl#L1-L7">source</a></section></article><h2 id="Perturbed-optimizers"><a class="docs-heading-anchor" href="#Perturbed-optimizers">Perturbed optimizers</a><a id="Perturbed-optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Perturbed-optimizers" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/2002.08676">Learning with Differentiable Perturbed Optimizers</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.AbstractPerturbed" href="#InferOpt.AbstractPerturbed"><code>InferOpt.AbstractPerturbed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractPerturbed{B}</code></pre><p>Differentiable perturbation of a black box optimizer. The parameter <code>parallel</code> is a boolean value, equal to true if the perturbations are run in parallel.</p><p><strong>Applicable functions</strong></p><ul><li><a href="#InferOpt.compute_probability_distribution"><code>compute_probability_distribution(perturbed::AbstractPerturbed, θ)</code></a></li><li><code>(perturbed::AbstractPerturbed)(θ)</code></li></ul><p><strong>Available subtypes</strong></p><ul><li><a href="#InferOpt.PerturbedAdditive"><code>PerturbedAdditive</code></a></li><li><a href="#InferOpt.PerturbedMultiplicative"><code>PerturbedMultiplicative</code></a></li></ul><p>These subtypes share the following fields:</p><ul><li><code>maximizer</code>: black box optimizer</li><li><code>ε</code>: magnitude of the perturbation</li><li><code>nb_samples::Int</code>: number of random samples for Monte-Carlo computations</li><li><code>rng::AbstractRNG</code>: random number generator</li><li><code>seed::Union{Nothing,Int}</code>: random seed</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/perturbed/abstract_perturbed.jl#L1-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.AbstractPerturbed-Tuple{AbstractArray{&lt;:Real}}" href="#InferOpt.AbstractPerturbed-Tuple{AbstractArray{&lt;:Real}}"><code>InferOpt.AbstractPerturbed</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(perturbed::AbstractPerturbed)(θ)</code></pre><p>Apply <code>compute_probability_distribution(perturbed, θ)</code> and return the expectation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/perturbed/abstract_perturbed.jl#L81-L85">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_probability_distribution-Tuple{InferOpt.AbstractPerturbed, AbstractArray{&lt;:Real}}" href="#InferOpt.compute_probability_distribution-Tuple{InferOpt.AbstractPerturbed, AbstractArray{&lt;:Real}}"><code>InferOpt.compute_probability_distribution</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compute_probability_distribution(perturbed::AbstractPerturbed, θ)</code></pre><p>Turn random perturbations of <code>θ</code> into a distribution on polytope vertices.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/perturbed/abstract_perturbed.jl#L69-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.sample_perturbations-Tuple{InferOpt.AbstractPerturbed, AbstractArray{&lt;:Real}}" href="#InferOpt.sample_perturbations-Tuple{InferOpt.AbstractPerturbed, AbstractArray{&lt;:Real}}"><code>InferOpt.sample_perturbations</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sample_perturbations(perturbed::AbstractPerturbed, θ)</code></pre><p>Draw random perturbations <code>Z</code> which will be applied to the objective direction <code>θ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/perturbed/abstract_perturbed.jl#L27-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedAdditive" href="#InferOpt.PerturbedAdditive"><code>InferOpt.PerturbedAdditive</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PerturbedAdditive{F}</code></pre><p>Differentiable normal perturbation of a black-box optimizer of type <code>F</code>: the input undergoes <code>θ -&gt; θ + εZ</code> where <code>Z ∼ N(0, I)</code>.</p><p>See also: <a href="#InferOpt.AbstractPerturbed"><code>AbstractPerturbed</code></a>.</p><p>Reference: <a href="https://arxiv.org/abs/2002.08676">https://arxiv.org/abs/2002.08676</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/perturbed/additive.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedAdditive-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}" href="#InferOpt.PerturbedAdditive-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}"><code>InferOpt.PerturbedAdditive</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">PerturbedAdditive(maximizer[; ε=1.0, nb_samples=1])</code></pre><p>Shorter constructor with defaults.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/perturbed/additive.jl#L33-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedMultiplicative" href="#InferOpt.PerturbedMultiplicative"><code>InferOpt.PerturbedMultiplicative</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PerturbedMultiplicative{F}</code></pre><p>Differentiable log-normal perturbation of a black-box optimizer of type <code>F</code>: the input undergoes <code>θ -&gt; θ ⊙ exp[εZ - ε²/2]</code> where <code>Z ∼ N(0, I)</code>.</p><p>See also: <a href="#InferOpt.AbstractPerturbed"><code>AbstractPerturbed</code></a>.</p><p>Reference: preprint coming soon.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/perturbed/multiplicative.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedMultiplicative-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}" href="#InferOpt.PerturbedMultiplicative-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}"><code>InferOpt.PerturbedMultiplicative</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">PerturbedMultiplicative(maximizer[; ε=1.0, nb_samples=1])</code></pre><p>Shorter constructor with defaults.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/perturbed/multiplicative.jl#L33-L37">source</a></section></article><h2 id="Fenchel-Young-losses"><a class="docs-heading-anchor" href="#Fenchel-Young-losses">Fenchel-Young losses</a><a id="Fenchel-Young-losses-1"></a><a class="docs-heading-anchor-permalink" href="#Fenchel-Young-losses" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/1901.02324">Learning with Fenchel-Young Losses</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.FenchelYoungLoss" href="#InferOpt.FenchelYoungLoss"><code>InferOpt.FenchelYoungLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FenchelYoungLoss{P}</code></pre><p>Fenchel-Young loss associated with a given regularized prediction function.</p><p><strong>Fields</strong></p><ul><li><code>predictor::P</code>: prediction function of the form <code>ŷ(θ) = argmax {θᵀy - Ω(y)}</code></li></ul><p>Reference: <a href="https://arxiv.org/abs/1901.02324">https://arxiv.org/abs/1901.02324</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/fenchel_young/fenchel_young.jl#L1-L10">source</a></section></article><h2 id="Generalized-imitation-losses"><a class="docs-heading-anchor" href="#Generalized-imitation-losses">Generalized imitation losses</a><a id="Generalized-imitation-losses-1"></a><a class="docs-heading-anchor-permalink" href="#Generalized-imitation-losses" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">Reference</header><div class="admonition-body"><p><a href="https://arxiv.org/abs/2207.13513">Learning with Combinatorial Optimization Layers: a Probabilistic Approach</a></p></div></div><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ImitationLoss" href="#InferOpt.ImitationLoss"><code>InferOpt.ImitationLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ImitationLoss{L,R,P}</code></pre><p>Generic imitation loss: max<em>y base</em>loss(y, t<em>true) + α θᵀ(y - y</em>true) - (Ω(y) - Ω(y_true))).</p><p>When <code>base_loss = 0</code>, this loss is equivalent to a <a href="#InferOpt.FenchelYoungLoss"><code>FenchelYoungLoss</code></a>. When <code>Ω = 0</code>, this loss is equivalent to the <a href="#InferOpt.StructuredSVMLoss"><code>StructuredSVMLoss</code></a>.</p><p><strong>Fields</strong></p><ul><li><code>maximizer::P</code>: function that computes   argmax<em>y base</em>loss(y, t<em>true) + α θᵀ(y - y</em>true) - (Ω(y) - Ω(y<em>true)), takes (θ, y</em>true, kwargs...)   or (θ, t_true, kwargs...) as input</li><li><code>base_loss::L</code>: base loss, takes (y, t_true) as input</li><li><code>Ω::R</code>: regularization, takes y as input</li><li><code>α::Float64</code>: default value of 1.0</li></ul><p>Note: by default, <code>t_true</code> is a named tuple with field <code>y_true</code>,     but can be any data structure for which the <a href="#InferOpt.get_y_true-Tuple{Any}"><code>get_y_true</code></a> method is implemented.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/imitation_loss/imitation_loss.jl#L1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ImitationLoss-Tuple{Any}" href="#InferOpt.ImitationLoss-Tuple{Any}"><code>InferOpt.ImitationLoss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ImitationLoss(maximizer[; base_loss=(y,t_true)-&gt;0.0, Ω=y-&gt;0.0, α=1.0])</code></pre><p>Shorter constructor with defaults.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/imitation_loss/imitation_loss.jl#L32-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.get_y_true-Tuple{Any}" href="#InferOpt.get_y_true-Tuple{Any}"><code>InferOpt.get_y_true</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_y_true(t_true::Any)</code></pre><p>Retrieve <code>y_true</code> from <code>t_true</code>. This method should be implemented when using a custom data structure for <code>t_true</code> other than a <code>NamedTuple</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/imitation_loss/imitation_loss.jl#L41-L46">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.get_y_true-Tuple{NamedTuple}" href="#InferOpt.get_y_true-Tuple{NamedTuple}"><code>InferOpt.get_y_true</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_y_true(t_true::NamedTuple)</code></pre><p>Retrieve <code>y_true</code> from <code>t_true</code>. <code>t_true</code> must contain an <code>y_true</code> field.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/7e65548ffe3c777089c91ca5f8bc96dbba623027/src/imitation_loss/imitation_loss.jl#L49-L53">source</a></section></article><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#InferOpt.DEFAULT_FRANK_WOLFE_KWARGS"><code>InferOpt.DEFAULT_FRANK_WOLFE_KWARGS</code></a></li><li><a href="#InferOpt.AbstractPerturbed-Tuple{AbstractArray{&lt;:Real}}"><code>InferOpt.AbstractPerturbed</code></a></li><li><a href="#InferOpt.AbstractPerturbed"><code>InferOpt.AbstractPerturbed</code></a></li><li><a href="#InferOpt.DifferentiableFrankWolfe-Tuple{AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}"><code>InferOpt.DifferentiableFrankWolfe</code></a></li><li><a href="#InferOpt.DifferentiableFrankWolfe"><code>InferOpt.DifferentiableFrankWolfe</code></a></li><li><a href="#InferOpt.FenchelYoungLoss"><code>InferOpt.FenchelYoungLoss</code></a></li><li><a href="#InferOpt.FixedAtomsProbabilityDistribution-Tuple{FrankWolfe.ActiveSet}"><code>InferOpt.FixedAtomsProbabilityDistribution</code></a></li><li><a href="#InferOpt.FixedAtomsProbabilityDistribution"><code>InferOpt.FixedAtomsProbabilityDistribution</code></a></li><li><a href="#InferOpt.ImitationLoss"><code>InferOpt.ImitationLoss</code></a></li><li><a href="#InferOpt.ImitationLoss-Tuple{Any}"><code>InferOpt.ImitationLoss</code></a></li><li><a href="#InferOpt.Interpolation"><code>InferOpt.Interpolation</code></a></li><li><a href="#InferOpt.IsBaseLoss"><code>InferOpt.IsBaseLoss</code></a></li><li><a href="#InferOpt.IsRegularized"><code>InferOpt.IsRegularized</code></a></li><li><a href="#InferOpt.LMOWrapper"><code>InferOpt.LMOWrapper</code></a></li><li><a href="#InferOpt.PerturbedAdditive"><code>InferOpt.PerturbedAdditive</code></a></li><li><a href="#InferOpt.PerturbedAdditive-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}"><code>InferOpt.PerturbedAdditive</code></a></li><li><a href="#InferOpt.PerturbedMultiplicative-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}"><code>InferOpt.PerturbedMultiplicative</code></a></li><li><a href="#InferOpt.PerturbedMultiplicative"><code>InferOpt.PerturbedMultiplicative</code></a></li><li><a href="#InferOpt.Pushforward"><code>InferOpt.Pushforward</code></a></li><li><a href="#InferOpt.Pushforward-Tuple{AbstractArray{&lt;:Real}}"><code>InferOpt.Pushforward</code></a></li><li><a href="#InferOpt.RegularizedGeneric-Tuple{Any}"><code>InferOpt.RegularizedGeneric</code></a></li><li><a href="#InferOpt.RegularizedGeneric-Tuple{AbstractArray{&lt;:Real}}"><code>InferOpt.RegularizedGeneric</code></a></li><li><a href="#InferOpt.RegularizedGeneric"><code>InferOpt.RegularizedGeneric</code></a></li><li><a href="#InferOpt.SPOPlusLoss"><code>InferOpt.SPOPlusLoss</code></a></li><li><a href="#InferOpt.StructuredSVMLoss"><code>InferOpt.StructuredSVMLoss</code></a></li><li><a href="#InferOpt.ZeroOneBaseLoss"><code>InferOpt.ZeroOneBaseLoss</code></a></li><li><a href="#InferOpt.apply_on_atoms-Tuple{Any, FixedAtomsProbabilityDistribution}"><code>InferOpt.apply_on_atoms</code></a></li><li><a href="#InferOpt.compress_distribution!-Union{Tuple{FixedAtomsProbabilityDistribution{A, W}}, Tuple{W}, Tuple{A}} where {A, W}"><code>InferOpt.compress_distribution!</code></a></li><li><a href="#InferOpt.compute_expectation"><code>InferOpt.compute_expectation</code></a></li><li><a href="#InferOpt.compute_maximizer-Union{Tuple{L}, Tuple{Type{IsBaseLoss{L}}, L, Any, Any, Any}} where L"><code>InferOpt.compute_maximizer</code></a></li><li><a href="#InferOpt.compute_probability_distribution-Tuple{InferOpt.AbstractPerturbed, AbstractArray{&lt;:Real}}"><code>InferOpt.compute_probability_distribution</code></a></li><li><a href="#InferOpt.compute_probability_distribution"><code>InferOpt.compute_probability_distribution</code></a></li><li><a href="#InferOpt.compute_probability_distribution-Tuple{Pushforward, Any}"><code>InferOpt.compute_probability_distribution</code></a></li><li><a href="#InferOpt.compute_probability_distribution-Tuple{DifferentiableFrankWolfe, AbstractArray{&lt;:Real}, AbstractArray{&lt;:Real}}"><code>InferOpt.compute_probability_distribution</code></a></li><li><a href="#InferOpt.compute_probability_distribution-Tuple{RegularizedGeneric, AbstractArray{&lt;:Real}}"><code>InferOpt.compute_probability_distribution</code></a></li><li><a href="#InferOpt.compute_regularization"><code>InferOpt.compute_regularization</code></a></li><li><a href="#InferOpt.get_y_true-Tuple{Any}"><code>InferOpt.get_y_true</code></a></li><li><a href="#InferOpt.get_y_true-Tuple{NamedTuple}"><code>InferOpt.get_y_true</code></a></li><li><a href="#InferOpt.half_square_norm-Tuple{AbstractArray{&lt;:Real}}"><code>InferOpt.half_square_norm</code></a></li><li><a href="#InferOpt.isproba-Tuple{Real}"><code>InferOpt.isproba</code></a></li><li><a href="#InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.isprobadist</code></a></li><li><a href="#InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.one_hot_argmax</code></a></li><li><a href="#InferOpt.positive_part-Tuple{Any}"><code>InferOpt.positive_part</code></a></li><li><a href="#InferOpt.ranking-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.ranking</code></a></li><li><a href="#InferOpt.sample_perturbations-Tuple{InferOpt.AbstractPerturbed, AbstractArray{&lt;:Real}}"><code>InferOpt.sample_perturbations</code></a></li><li><a href="#InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.shannon_entropy</code></a></li><li><a href="#InferOpt.simplex_projection_and_support-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.simplex_projection_and_support</code></a></li><li><a href="#InferOpt.soft_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.soft_argmax</code></a></li><li><a href="#InferOpt.sparse_argmax-Tuple{AbstractVector{&lt;:Real}}"><code>InferOpt.sparse_argmax</code></a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../background/">« Background</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Thursday 13 April 2023 14:02">Thursday 13 April 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
