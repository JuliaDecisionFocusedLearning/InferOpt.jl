<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API reference · InferOpt.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://axelparmentier.github.io/InferOpt.jl/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">InferOpt.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../background/">Background</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../tutorial/">Basic tutorial</a></li><li><a class="tocitem" href="../advanced_applications/">Advanced applications</a></li></ul></li><li><span class="tocitem">Algorithms</span><ul><li><a class="tocitem" href="../optim/">Optimization</a></li><li><a class="tocitem" href="../losses/">Losses</a></li></ul></li><li class="is-active"><a class="tocitem" href>API reference</a><ul class="internal"><li><a class="tocitem" href="#Types"><span>Types</span></a></li><li><a class="tocitem" href="#Functions"><span>Functions</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/axelparmentier/InferOpt.jl/blob/main/docs/src/api.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="InferOpt" href="#InferOpt"><code>InferOpt</code></a> — <span class="docstring-category">Module</span></header><section><div><pre><code class="language-julia hljs">InferOpt</code></pre><p>A toolbox for using combinatorial optimization algorithms within machine learning pipelines.</p><p>See our preprint <a href="https://arxiv.org/abs/2207.13513">https://arxiv.org/abs/2207.13513</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/InferOpt.jl#L1-L7">source</a></section></article><h2 id="Types"><a class="docs-heading-anchor" href="#Types">Types</a><a id="Types-1"></a><a class="docs-heading-anchor-permalink" href="#Types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="InferOpt.AbstractLayer" href="#InferOpt.AbstractLayer"><code>InferOpt.AbstractLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractLayer</code></pre><p>Supertype for all the layers defined in InferOpt.</p><p>All of these layers are callable, and differentiable with any ChainRules-compatible autodiff backend.</p><p><strong>Interface</strong></p><ul><li><code>(layer::AbstractLayer)(args...; kwargs...)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/interface.jl#L1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.AbstractLossLayer" href="#InferOpt.AbstractLossLayer"><code>InferOpt.AbstractLossLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractLossLayer &lt;: AbstractLayer</code></pre><p>Supertype for all the loss layers defined in InferOpt.</p><p>Depending on the precise loss, the arguments to the layer might vary</p><p><strong>Interface</strong></p><ul><li><code>(layer::AbstractLossLayer)(θ; kwargs...)</code> or</li><li><code>(layer::AbstractLossLayer)(θ, θ_true; kwargs...)</code> or</li><li><code>(layer::AbstractLossLayer)(θ, y_true; kwargs...)</code> or</li><li><code>(layer::AbstractLossLayer)(θ, (; θ_true, y_true); kwargs...)</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/interface.jl#L28-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.AbstractOptimizationLayer" href="#InferOpt.AbstractOptimizationLayer"><code>InferOpt.AbstractOptimizationLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractOptimizationLayer &lt;: AbstractLayer</code></pre><p>Supertype for all the optimization layers defined in InferOpt.</p><p><strong>Interface</strong></p><ul><li><code>(layer::AbstractOptimizationLayer)(θ; kwargs...)</code></li><li><code>compute_probability_distribution(layer, θ; kwargs...)</code> (only if the layer is probabilistic)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/interface.jl#L15-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.AbstractPerturbed" href="#InferOpt.AbstractPerturbed"><code>InferOpt.AbstractPerturbed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractPerturbed{parallel} &lt;: AbstractOptimizationLayer</code></pre><p>Differentiable perturbation of a black box optimizer.</p><p>The parameter <code>parallel</code> is a boolean value, equal to true if the perturbations are run in parallel.</p><p><strong>Available implementations</strong></p><ul><li><a href="#InferOpt.PerturbedAdditive"><code>PerturbedAdditive</code></a></li><li><a href="#InferOpt.PerturbedMultiplicative"><code>PerturbedMultiplicative</code></a></li></ul><p>These two subtypes share the following fields:</p><ul><li><code>maximizer</code>: black box optimizer</li><li><code>ε</code>: magnitude of the perturbation</li><li><code>nb_samples::Int</code>: number of random samples for Monte-Carlo computations</li><li><code>rng::AbstractRNG</code>: random number generator</li><li><code>seed::Union{Nothing,Int}</code>: random seed</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/perturbed/abstract_perturbed.jl#L1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.AbstractPerturbed-Tuple{AbstractArray}" href="#InferOpt.AbstractPerturbed-Tuple{AbstractArray}"><code>InferOpt.AbstractPerturbed</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(perturbed::AbstractPerturbed)(θ; kwargs...)</code></pre><p>Apply <code>compute_probability_distribution(perturbed, θ; kwargs...)</code> and return the expectation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/perturbed/abstract_perturbed.jl#L79-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.AbstractRegularized" href="#InferOpt.AbstractRegularized"><code>InferOpt.AbstractRegularized</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractRegularized{parallel} &lt;: AbstractOptimizationLayer</code></pre><p>Convex regularization perturbation of a black box optimizer</p><pre><code class="nohighlight hljs">ŷ(θ) = argmax_{y ∈ C} {θᵀy - Ω(y)}</code></pre><p><strong>Interface</strong></p><ul><li><code>(regularized::AbstractRegularized)(θ; kwargs...)</code>: return <code>ŷ(θ)</code></li><li><code>compute_regularization(regularized, y)</code>: return <code>Ω(y)</code></li></ul><p><strong>Available implementations</strong></p><ul><li><a href="#InferOpt.SoftArgmax"><code>SoftArgmax</code></a></li><li><a href="#InferOpt.SparseArgmax"><code>SparseArgmax</code></a></li><li><a href="#InferOpt.RegularizedFrankWolfe"><code>RegularizedFrankWolfe</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/regularized/abstract_regularized.jl#L1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.FenchelYoungLoss" href="#InferOpt.FenchelYoungLoss"><code>InferOpt.FenchelYoungLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FenchelYoungLoss &lt;: AbstractLossLayer</code></pre><p>Fenchel-Young loss associated with a given optimization layer.</p><pre><code class="nohighlight hljs">L(θ, y_true) = (Ω(y_true) - θᵀy_true) - (Ω(ŷ) - θᵀŷ)</code></pre><p>Reference: <a href="https://arxiv.org/abs/1901.02324">https://arxiv.org/abs/1901.02324</a></p><p><strong>Fields</strong></p><ul><li><code>optimization_layer::AbstractOptimizationLayer</code>: optimization layer that can be formulated as <code>ŷ(θ) = argmax {θᵀy - Ω(y)}</code> (either regularized or perturbed)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/fenchel_young_loss.jl#L1-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.FenchelYoungLoss-Tuple{AbstractArray, AbstractArray}" href="#InferOpt.FenchelYoungLoss-Tuple{AbstractArray, AbstractArray}"><code>InferOpt.FenchelYoungLoss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(fyl::FenchelYoungLoss)(θ, y_true; kwargs...)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/fenchel_young_loss.jl#L26-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.FixedAtomsProbabilityDistribution" href="#InferOpt.FixedAtomsProbabilityDistribution"><code>InferOpt.FixedAtomsProbabilityDistribution</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FixedAtomsProbabilityDistribution{A,W}</code></pre><p>Encodes a probability distribution with finite support and fixed atoms.</p><p>See <a href="#InferOpt.compute_expectation"><code>compute_expectation</code></a> to understand the name of this struct.</p><p><strong>Fields</strong></p><ul><li><code>atoms::Vector{A}</code>: elements of the support</li><li><code>weights::Vector{W}</code>: probability values for each atom (must sum to 1)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/probability_distribution.jl#L1-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.IdentityRelaxation" href="#InferOpt.IdentityRelaxation"><code>InferOpt.IdentityRelaxation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">IdentityRelaxation &lt;: AbstractOptimizationLayer</code></pre><p>Naive relaxation of a black-box optimizer where constraints are simply forgotten.</p><p>Consider (centering and) normalizing <code>θ</code> before applying it.</p><p><strong>Fields</strong></p><ul><li><code>maximizer</code>: underlying argmax function</li></ul><p>Reference: <a href="https://arxiv.org/abs/2205.15213">https://arxiv.org/abs/2205.15213</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/simple/identity.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ImitationLoss" href="#InferOpt.ImitationLoss"><code>InferOpt.ImitationLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ImitationLoss &lt;: AbstractLossLayer</code></pre><p>Generic imitation loss of the form</p><pre><code class="nohighlight hljs">L(θ, t_true) = max_y {δ(y, t_true) + α θᵀ(y - y_true) - (Ω(y) - Ω(y_true))}</code></pre><ul><li>When <code>δ</code> is zero, this is equivalent to a <a href="#InferOpt.FenchelYoungLoss"><code>FenchelYoungLoss</code></a>.</li><li>When <code>Ω</code> is zero, this is equivalent to a <a href="#InferOpt.StructuredSVMLoss"><code>StructuredSVMLoss</code></a>.</li></ul><p>Note: by default, <code>t_true</code> is a named tuple with field <code>y_true</code>, but it can be any data structure for which the <a href="#InferOpt.get_y_true"><code>get_y_true</code></a> method is implemented.</p><p><strong>Fields</strong></p><ul><li><code>aux_loss_maximizer</code>: function of <code>(θ, t_true, α)</code> that computes the argmax in the problem above</li><li><code>δ</code>: base loss function</li><li><code>Ω</code>: regularization function</li><li><code>α::Float64</code>: hyperparameter with a default value of 1.0</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/imitation_loss.jl#L1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ImitationLoss-Tuple{AbstractArray, Any}" href="#InferOpt.ImitationLoss-Tuple{AbstractArray, Any}"><code>InferOpt.ImitationLoss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(il::ImitationLoss)(θ, t_true; kwargs...)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/imitation_loss.jl#L68-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ImitationLoss-Tuple{}" href="#InferOpt.ImitationLoss-Tuple{}"><code>InferOpt.ImitationLoss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ImitationLoss(; aux_loss_maximizer, δ, Ω, α=1.0)</code></pre><p>Explicit constructor with keyword arguments.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/imitation_loss.jl#L33-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.Interpolation" href="#InferOpt.Interpolation"><code>InferOpt.Interpolation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Interpolation &lt;: AbstractOptimizationLayer</code></pre><p>Piecewise-linear interpolation of a black-box optimizer.</p><p><strong>Fields</strong></p><ul><li><code>maximizer</code>: underlying argmax function</li><li><code>λ::Float64</code>: smoothing parameter (smaller = more faithful approximation, larger = more informative gradients)</li></ul><p>Reference: <a href="https://arxiv.org/abs/1912.02175">https://arxiv.org/abs/1912.02175</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/simple/interpolation.jl#L1-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedAdditive" href="#InferOpt.PerturbedAdditive"><code>InferOpt.PerturbedAdditive</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PerturbedAdditive &lt;: AbstractPerturbed</code></pre><p>Differentiable normal perturbation of a black-box maximizer: the input undergoes <code>θ -&gt; θ + εZ</code> where <code>Z ∼ N(0, I)</code>.</p><p>Reference: <a href="https://arxiv.org/abs/2002.08676">https://arxiv.org/abs/2002.08676</a></p><p>See <a href="#InferOpt.AbstractPerturbed"><code>AbstractPerturbed</code></a> for more details.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/perturbed/additive.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedAdditive-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}" href="#InferOpt.PerturbedAdditive-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}"><code>InferOpt.PerturbedAdditive</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">PerturbedAdditive(maximizer[; ε=1.0, nb_samples=1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/perturbed/additive.jl#L33-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedMultiplicative" href="#InferOpt.PerturbedMultiplicative"><code>InferOpt.PerturbedMultiplicative</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PerturbedMultiplicative &lt;: AbstractPerturbed</code></pre><p>Differentiable log-normal perturbation of a black-box maximizer: the input undergoes <code>θ -&gt; θ ⊙ exp[εZ - ε²/2]</code> where <code>Z ∼ N(0, I)</code>.</p><p>Reference: <a href="https://arxiv.org/abs/2207.13513">https://arxiv.org/abs/2207.13513</a></p><p>See <a href="#InferOpt.AbstractPerturbed"><code>AbstractPerturbed</code></a> for more details.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/perturbed/multiplicative.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.PerturbedMultiplicative-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}" href="#InferOpt.PerturbedMultiplicative-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}"><code>InferOpt.PerturbedMultiplicative</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">PerturbedMultiplicative(maximizer[; ε=1.0, nb_samples=1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/perturbed/multiplicative.jl#L33-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.Pushforward" href="#InferOpt.Pushforward"><code>InferOpt.Pushforward</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Pushforward &lt;: AbstractLayer</code></pre><p>Differentiable pushforward of a probabilistic optimization layer with an arbitrary function post-processing function.</p><p><code>Pushforward</code> can be used for direct regret minimization (aka learning by experience) when the post-processing returns a cost.</p><p><strong>Fields</strong></p><ul><li><code>optimization_layer::AbstractOptimizationLayer</code>: probabilistic optimization layer</li><li><code>post_processing</code>: callable</li></ul><p>See also: <a href="#InferOpt.FixedAtomsProbabilityDistribution"><code>FixedAtomsProbabilityDistribution</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/pushforward.jl#L1-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.Pushforward-Tuple{AbstractArray}" href="#InferOpt.Pushforward-Tuple{AbstractArray}"><code>InferOpt.Pushforward</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(pushforward::Pushforward)(θ; kwargs...)</code></pre><p>Output the expectation of <code>pushforward.post_processing(X)</code>, where <code>X</code> follows the distribution defined by <code>pushforward.optimization_layer</code> applied to <code>θ</code>.</p><p>Unlike <a href="#InferOpt.compute_probability_distribution"><code>compute_probability_distribution(pushforward, θ)</code></a>, this function is differentiable, even if <code>pushforward.post_processing</code> isn&#39;t.</p><p>See also: <a href="#InferOpt.compute_expectation"><code>compute_expectation</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/pushforward.jl#L40-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.RegularizedFrankWolfe" href="#InferOpt.RegularizedFrankWolfe"><code>InferOpt.RegularizedFrankWolfe</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RegularizedFrankWolfe &lt;: AbstractRegularized</code></pre><p>Regularized optimization layer which relies on the Frank-Wolfe algorithm to define a probability distribution while solving</p><pre><code class="nohighlight hljs">ŷ(θ) = argmax_{y ∈ C} {θᵀy - Ω(y)}</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Since this is a conditional dependency, you need to have loaded the package DifferentiableFrankWolfe.jl before using <code>RegularizedFrankWolfe</code>.</p></div></div><p><strong>Fields</strong></p><ul><li><code>linear_maximizer</code>: linear maximization oracle <code>θ -&gt; argmax_{x ∈ C} θᵀx</code>, implicitly defines the polytope <code>C</code></li><li><code>Ω</code>: regularization function <code>Ω(y)</code></li><li><code>Ω_grad</code>: gradient function of the regularization function <code>∇Ω(y)</code></li><li><code>frank_wolfe_kwargs</code>: named tuple of keyword arguments passed to the Frank-Wolfe algorithm</li></ul><p><strong>Frank-Wolfe parameters</strong></p><p>Some values you can tune:</p><ul><li><code>epsilon::Float64</code>: precision target</li><li><code>max_iteration::Integer</code>: max number of iterations</li><li><code>timeout::Float64</code>: max runtime in seconds</li><li><code>lazy::Bool</code>: caching strategy</li><li><code>away_steps::Bool</code>: avoid zig-zagging</li><li><code>line_search::FrankWolfe.LineSearchMethod</code>: step size selection</li><li><code>verbose::Bool</code>: console output</li></ul><p>See the documentation of FrankWolfe.jl for details.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/regularized/regularized_frank_wolfe.jl#L1-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.RegularizedFrankWolfe-Tuple{AbstractArray}" href="#InferOpt.RegularizedFrankWolfe-Tuple{AbstractArray}"><code>InferOpt.RegularizedFrankWolfe</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(regularized::RegularizedFrankWolfe)(θ; kwargs...)</code></pre><p>Apply <code>compute_probability_distribution(regularized, θ; kwargs...)</code> and return the expectation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/regularized/regularized_frank_wolfe.jl#L57-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.RegularizedFrankWolfe-Tuple{Any}" href="#InferOpt.RegularizedFrankWolfe-Tuple{Any}"><code>InferOpt.RegularizedFrankWolfe</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">RegularizedFrankWolfe(linear_maximizer; Ω, Ω_grad, frank_wolfe_kwargs=(;))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/regularized/regularized_frank_wolfe.jl#L39-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.SPOPlusLoss" href="#InferOpt.SPOPlusLoss"><code>InferOpt.SPOPlusLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SPOPlusLoss &lt;: AbstractLossLayer</code></pre><p>Convex surrogate of the Smart &quot;Predict-then-Optimize&quot; loss.</p><p><strong>Fields</strong></p><ul><li><code>maximizer</code>: linear maximizer function of the form <code>θ -&gt; ŷ(θ) = argmax θᵀy</code></li><li><code>α::Float64</code>: convexification parameter, default = 2.0</li></ul><p>Reference: <a href="https://arxiv.org/abs/1710.08005">https://arxiv.org/abs/1710.08005</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/spoplus_loss.jl#L1-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.SPOPlusLoss-Tuple{AbstractArray, AbstractArray, AbstractArray}" href="#InferOpt.SPOPlusLoss-Tuple{AbstractArray, AbstractArray, AbstractArray}"><code>InferOpt.SPOPlusLoss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(spol::SPOPlusLoss)(θ, θ_true, y_true; kwargs...)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/spoplus_loss.jl#L29-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.SPOPlusLoss-Tuple{AbstractArray, AbstractArray}" href="#InferOpt.SPOPlusLoss-Tuple{AbstractArray, AbstractArray}"><code>InferOpt.SPOPlusLoss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(spol::SPOPlusLoss)(θ, θ_true; kwargs...)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/spoplus_loss.jl#L42-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.SPOPlusLoss-Tuple{Any}" href="#InferOpt.SPOPlusLoss-Tuple{Any}"><code>InferOpt.SPOPlusLoss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">SPOPlusLoss(maximizer; α=2.0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/spoplus_loss.jl#L22-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.SoftArgmax" href="#InferOpt.SoftArgmax"><code>InferOpt.SoftArgmax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SoftArgmax &lt;: Regularized</code></pre><p>Soft argmax activation function <code>s(z) = (e^zᵢ / ∑ e^zⱼ)ᵢ</code>.</p><p>Corresponds to regularized prediction on the probability simplex with entropic penalty.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/regularized/soft_argmax.jl#L1-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.SparseArgmax" href="#InferOpt.SparseArgmax"><code>InferOpt.SparseArgmax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SparseArgmax &lt;: AbstractRegularized</code></pre><p>Compute the Euclidean projection of the vector <code>z</code> onto the probability simplex.</p><p>Corresponds to regularized prediction on the probability simplex with square norm penalty.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/regularized/sparse_argmax.jl#L1-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.StructuredSVMLoss" href="#InferOpt.StructuredSVMLoss"><code>InferOpt.StructuredSVMLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StructuredSVMLoss &lt;: AbstractLossLayer</code></pre><p>Loss associated with the Structured Support Vector Machine, defined by</p><pre><code class="nohighlight hljs">L(θ, y_true) = max_y {δ(y, y_true) + α θᵀ(y - y_true)}</code></pre><p>Reference: <a href="http://www.nowozin.net/sebastian/papers/nowozin2011structured-tutorial.pdf">http://www.nowozin.net/sebastian/papers/nowozin2011structured-tutorial.pdf</a> (Chapter 6)</p><p><strong>Fields</strong></p><ul><li><code>aux_loss_maximizer::M</code>: function of <code>(θ, y_true, α)</code> that computes the argmax in the problem above</li><li><code>δ::L</code>: base loss function</li><li><code>α::Float64</code>: hyperparameter with a default value of 1.0</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/ssvm_loss.jl#L1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.StructuredSVMLoss-Tuple{AbstractArray, AbstractArray}" href="#InferOpt.StructuredSVMLoss-Tuple{AbstractArray, AbstractArray}"><code>InferOpt.StructuredSVMLoss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(ssvml::StructuredSVMLoss)(θ, y_true; kwargs...)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/ssvm_loss.jl#L46-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.StructuredSVMLoss-Tuple{}" href="#InferOpt.StructuredSVMLoss-Tuple{}"><code>InferOpt.StructuredSVMLoss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">StructuredSVMLoss(; aux_loss_maximizer, δ, α=1.0)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/ssvm_loss.jl#L23-L25">source</a></section></article><h2 id="Functions"><a class="docs-heading-anchor" href="#Functions">Functions</a><a id="Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Base.rand-Tuple{Random.AbstractRNG, FixedAtomsProbabilityDistribution}" href="#Base.rand-Tuple{Random.AbstractRNG, FixedAtomsProbabilityDistribution}"><code>Base.rand</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rand([rng,] probadist)</code></pre><p>Sample from the atoms of <code>probadist</code> according to their weights.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/probability_distribution.jl#L27-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ZeroOneImitationLoss" href="#InferOpt.ZeroOneImitationLoss"><code>InferOpt.ZeroOneImitationLoss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ZeroOneStructuredSVMLoss(α)</code></pre><p>Implementation of the <a href="#InferOpt.ImitationLoss"><code>ImitationLoss</code></a> based on a 0-1 loss for multiclass classification with no regularization.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/zero_one_loss.jl#L45-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ZeroOneStructuredSVMLoss" href="#InferOpt.ZeroOneStructuredSVMLoss"><code>InferOpt.ZeroOneStructuredSVMLoss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ZeroOneStructuredSVMLoss</code></pre><p>Implementation of the <a href="#InferOpt.StructuredSVMLoss"><code>StructuredSVMLoss</code></a> based on a 0-1 loss for multiclass classification.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/zero_one_loss.jl#L34-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.apply_on_atoms-Tuple{Any, FixedAtomsProbabilityDistribution}" href="#InferOpt.apply_on_atoms-Tuple{Any, FixedAtomsProbabilityDistribution}"><code>InferOpt.apply_on_atoms</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">apply_on_atoms(post_processing, probadist)</code></pre><p>Create a new distribution by applying the function <code>post_processing</code> to each atom of <code>probadist</code> (the weights remain the same).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/probability_distribution.jl#L39-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_expectation" href="#InferOpt.compute_expectation"><code>InferOpt.compute_expectation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">compute_expectation(probadist[, post_processing=identity])</code></pre><p>Compute the expectation of <code>post_processing(X)</code> where <code>X</code> is a random variable distributed according to <code>probadist</code>.</p><p>This operation is made differentiable thanks to a custom reverse rule, even when <code>post_processing</code> itself is not a differentiable function.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Derivatives are computed with respect to <code>probadist.weights</code> only, assuming that <code>probadist.atoms</code> doesn&#39;t change (hence the name <a href="#InferOpt.FixedAtomsProbabilityDistribution"><code>FixedAtomsProbabilityDistribution</code></a>).</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/probability_distribution.jl#L52-L61">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_probability_distribution" href="#InferOpt.compute_probability_distribution"><code>InferOpt.compute_probability_distribution</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">compute_probability_distribution(layer, θ; kwargs...)</code></pre><p>Apply a probabilistic optimization layer to an objective direction <code>θ</code> in order to generate a <a href="#InferOpt.FixedAtomsProbabilityDistribution"><code>FixedAtomsProbabilityDistribution</code></a> on the vertices of a polytope.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/interface.jl#L45-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_probability_distribution-Tuple{AbstractPerturbed, AbstractArray}" href="#InferOpt.compute_probability_distribution-Tuple{AbstractPerturbed, AbstractArray}"><code>InferOpt.compute_probability_distribution</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compute_probability_distribution(perturbed::AbstractPerturbed, θ; kwargs...)</code></pre><p>Turn random perturbations of <code>θ</code> into a distribution on polytope vertices.</p><p>Keyword arguments are passed to the underlying linear maximizer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/perturbed/abstract_perturbed.jl#L65-L71">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_probability_distribution-Tuple{Pushforward, Any}" href="#InferOpt.compute_probability_distribution-Tuple{Pushforward, Any}"><code>InferOpt.compute_probability_distribution</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">compute_probability_distribution(pushforward, θ)</code></pre><p>Output the distribution of <code>pushforward.post_processing(X)</code>, where <code>X</code> follows the distribution defined by <code>pushforward.optimization_layer</code> applied to <code>θ</code>.</p><p>This function is not differentiable if <code>pushforward.post_processing</code> isn&#39;t.</p><p>See also: <a href="#InferOpt.apply_on_atoms-Tuple{Any, FixedAtomsProbabilityDistribution}"><code>apply_on_atoms</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/pushforward.jl#L24-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.compute_regularization" href="#InferOpt.compute_regularization"><code>InferOpt.compute_regularization</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">compute_regularization(regularized, y)</code></pre><p>Return the convex penalty <code>Ω(y)</code> associated with an <code>AbstractRegularized</code> layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/regularized/abstract_regularized.jl#L22-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.get_y_true" href="#InferOpt.get_y_true"><code>InferOpt.get_y_true</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">get_y_true(t_true::Any)</code></pre><p>Retrieve <code>y_true</code> from <code>t_true</code>.</p><p>This method should be implemented when using a custom data structure for <code>t_true</code> other than a <code>NamedTuple</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/imitation_loss.jl#L42-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.get_y_true-Tuple{NamedTuple}" href="#InferOpt.get_y_true-Tuple{NamedTuple}"><code>InferOpt.get_y_true</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_y_true(t_true::NamedTuple)</code></pre><p>Retrieve <code>y_true</code> from <code>t_true</code>. <code>t_true</code> must contain an <code>y_true</code> field.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/imitation_loss.jl#L51-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.half_square_norm-Tuple{AbstractArray}" href="#InferOpt.half_square_norm-Tuple{AbstractArray}"><code>InferOpt.half_square_norm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">half_square_norm(x)</code></pre><p>Compute the squared Euclidean norm of <code>x</code> and divide it by 2.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/some_functions.jl#L22-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.isproba-Tuple{Real}" href="#InferOpt.isproba-Tuple{Real}"><code>InferOpt.isproba</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">isproba(x)</code></pre><p>Check whether <code>x ∈ [0,1]</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/some_functions.jl#L8-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real" href="#InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.isprobadist</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">isprobadist(p)</code></pre><p>Check whether the elements of <code>p</code> are nonnegative and sum to 1.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/some_functions.jl#L15-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real" href="#InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.one_hot_argmax</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">one_hot_argmax(z)</code></pre><p>One-hot encoding of the argmax function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/some_functions.jl#L48-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.positive_part-Tuple{Any}" href="#InferOpt.positive_part-Tuple{Any}"><code>InferOpt.positive_part</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">positive_part(x)</code></pre><p>Compute <code>max(x,0)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/some_functions.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.ranking-Tuple{AbstractVector}" href="#InferOpt.ranking-Tuple{AbstractVector}"><code>InferOpt.ranking</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ranking(θ[; rev])</code></pre><p>Compute the vector <code>r</code> such that <code>rᵢ</code> is the rank of <code>θᵢ</code> in <code>θ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/some_functions.jl#L59-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.sample_perturbations-Tuple{AbstractPerturbed, AbstractArray}" href="#InferOpt.sample_perturbations-Tuple{AbstractPerturbed, AbstractArray}"><code>InferOpt.sample_perturbations</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sample_perturbations(perturbed::AbstractPerturbed, θ)</code></pre><p>Draw random perturbations <code>Z</code> which will be applied to the objective direction <code>θ</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/perturbed/abstract_perturbed.jl#L23-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real" href="#InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.shannon_entropy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">shannon_entropy(p)</code></pre><p>Compute the Shannon entropy of a probability distribution: <code>H(p) = -∑ pᵢlog(pᵢ)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/utils/some_functions.jl#L31-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.simplex_projection_and_support-Tuple{AbstractVector}" href="#InferOpt.simplex_projection_and_support-Tuple{AbstractVector}"><code>InferOpt.simplex_projection_and_support</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">simplex_projection_and_support(z)</code></pre><p>Compute the Euclidean projection <code>p</code> of <code>z</code> on the probability simplex (also called <code>sparse_argmax</code>), and the indicators <code>s</code> of its support.</p><p>Reference: <a href="https://arxiv.org/abs/1602.02068">https://arxiv.org/abs/1602.02068</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/regularized/sparse_argmax.jl#L22-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.zero_one_loss-Tuple{AbstractArray, AbstractArray}" href="#InferOpt.zero_one_loss-Tuple{AbstractArray, AbstractArray}"><code>InferOpt.zero_one_loss</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">zero_one_loss(y, y_true)</code></pre><p>0-1 loss for multiclass classification: <code>δ(y, y_true) = 0</code> if <code>y = y_true</code>, and <code>1</code> otherwise.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/zero_one_loss.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InferOpt.zero_one_loss_maximizer-Union{Tuple{R}, Tuple{AbstractVector, AbstractVector{R}, Any}} where R&lt;:Real" href="#InferOpt.zero_one_loss_maximizer-Union{Tuple{R}, Tuple{AbstractVector, AbstractVector{R}, Any}} where R&lt;:Real"><code>InferOpt.zero_one_loss_maximizer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">zero_one_loss_maximizer(y, y_true; α)</code></pre><p>For <code>δ = zero_one_loss</code>, compute</p><pre><code class="nohighlight hljs">argmax_y {δ(y, y_true) + α θᵀ(y - y_true)}</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/axelparmentier/InferOpt.jl/blob/98ea7ccde693460f3caca9f11ef79eaa0855e14a/src/imitation/zero_one_loss.jl#L10-L17">source</a></section></article><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#InferOpt.AbstractLayer"><code>InferOpt.AbstractLayer</code></a></li><li><a href="#InferOpt.AbstractLossLayer"><code>InferOpt.AbstractLossLayer</code></a></li><li><a href="#InferOpt.AbstractOptimizationLayer"><code>InferOpt.AbstractOptimizationLayer</code></a></li><li><a href="#InferOpt.AbstractPerturbed"><code>InferOpt.AbstractPerturbed</code></a></li><li><a href="#InferOpt.AbstractPerturbed-Tuple{AbstractArray}"><code>InferOpt.AbstractPerturbed</code></a></li><li><a href="#InferOpt.AbstractRegularized"><code>InferOpt.AbstractRegularized</code></a></li><li><a href="#InferOpt.FenchelYoungLoss-Tuple{AbstractArray, AbstractArray}"><code>InferOpt.FenchelYoungLoss</code></a></li><li><a href="#InferOpt.FenchelYoungLoss"><code>InferOpt.FenchelYoungLoss</code></a></li><li><a href="#InferOpt.FixedAtomsProbabilityDistribution"><code>InferOpt.FixedAtomsProbabilityDistribution</code></a></li><li><a href="#InferOpt.IdentityRelaxation"><code>InferOpt.IdentityRelaxation</code></a></li><li><a href="#InferOpt.ImitationLoss-Tuple{AbstractArray, Any}"><code>InferOpt.ImitationLoss</code></a></li><li><a href="#InferOpt.ImitationLoss"><code>InferOpt.ImitationLoss</code></a></li><li><a href="#InferOpt.ImitationLoss-Tuple{}"><code>InferOpt.ImitationLoss</code></a></li><li><a href="#InferOpt.Interpolation"><code>InferOpt.Interpolation</code></a></li><li><a href="#InferOpt.PerturbedAdditive-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}"><code>InferOpt.PerturbedAdditive</code></a></li><li><a href="#InferOpt.PerturbedAdditive"><code>InferOpt.PerturbedAdditive</code></a></li><li><a href="#InferOpt.PerturbedMultiplicative"><code>InferOpt.PerturbedMultiplicative</code></a></li><li><a href="#InferOpt.PerturbedMultiplicative-Union{Tuple{S}, Tuple{R}, Tuple{F}} where {F, R, S}"><code>InferOpt.PerturbedMultiplicative</code></a></li><li><a href="#InferOpt.Pushforward-Tuple{AbstractArray}"><code>InferOpt.Pushforward</code></a></li><li><a href="#InferOpt.Pushforward"><code>InferOpt.Pushforward</code></a></li><li><a href="#InferOpt.RegularizedFrankWolfe"><code>InferOpt.RegularizedFrankWolfe</code></a></li><li><a href="#InferOpt.RegularizedFrankWolfe-Tuple{Any}"><code>InferOpt.RegularizedFrankWolfe</code></a></li><li><a href="#InferOpt.RegularizedFrankWolfe-Tuple{AbstractArray}"><code>InferOpt.RegularizedFrankWolfe</code></a></li><li><a href="#InferOpt.SPOPlusLoss"><code>InferOpt.SPOPlusLoss</code></a></li><li><a href="#InferOpt.SPOPlusLoss-Tuple{Any}"><code>InferOpt.SPOPlusLoss</code></a></li><li><a href="#InferOpt.SPOPlusLoss-Tuple{AbstractArray, AbstractArray, AbstractArray}"><code>InferOpt.SPOPlusLoss</code></a></li><li><a href="#InferOpt.SPOPlusLoss-Tuple{AbstractArray, AbstractArray}"><code>InferOpt.SPOPlusLoss</code></a></li><li><a href="#InferOpt.SoftArgmax"><code>InferOpt.SoftArgmax</code></a></li><li><a href="#InferOpt.SparseArgmax"><code>InferOpt.SparseArgmax</code></a></li><li><a href="#InferOpt.StructuredSVMLoss-Tuple{AbstractArray, AbstractArray}"><code>InferOpt.StructuredSVMLoss</code></a></li><li><a href="#InferOpt.StructuredSVMLoss-Tuple{}"><code>InferOpt.StructuredSVMLoss</code></a></li><li><a href="#InferOpt.StructuredSVMLoss"><code>InferOpt.StructuredSVMLoss</code></a></li><li><a href="#InferOpt.ZeroOneImitationLoss"><code>InferOpt.ZeroOneImitationLoss</code></a></li><li><a href="#InferOpt.ZeroOneStructuredSVMLoss"><code>InferOpt.ZeroOneStructuredSVMLoss</code></a></li><li><a href="#InferOpt.apply_on_atoms-Tuple{Any, FixedAtomsProbabilityDistribution}"><code>InferOpt.apply_on_atoms</code></a></li><li><a href="#InferOpt.compute_expectation"><code>InferOpt.compute_expectation</code></a></li><li><a href="#InferOpt.compute_probability_distribution"><code>InferOpt.compute_probability_distribution</code></a></li><li><a href="#InferOpt.compute_probability_distribution-Tuple{AbstractPerturbed, AbstractArray}"><code>InferOpt.compute_probability_distribution</code></a></li><li><a href="#InferOpt.compute_probability_distribution-Tuple{Pushforward, Any}"><code>InferOpt.compute_probability_distribution</code></a></li><li><a href="#InferOpt.compute_regularization"><code>InferOpt.compute_regularization</code></a></li><li><a href="#InferOpt.get_y_true-Tuple{NamedTuple}"><code>InferOpt.get_y_true</code></a></li><li><a href="#InferOpt.get_y_true"><code>InferOpt.get_y_true</code></a></li><li><a href="#InferOpt.half_square_norm-Tuple{AbstractArray}"><code>InferOpt.half_square_norm</code></a></li><li><a href="#InferOpt.isproba-Tuple{Real}"><code>InferOpt.isproba</code></a></li><li><a href="#InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.isprobadist</code></a></li><li><a href="#InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.one_hot_argmax</code></a></li><li><a href="#InferOpt.positive_part-Tuple{Any}"><code>InferOpt.positive_part</code></a></li><li><a href="#InferOpt.ranking-Tuple{AbstractVector}"><code>InferOpt.ranking</code></a></li><li><a href="#InferOpt.sample_perturbations-Tuple{AbstractPerturbed, AbstractArray}"><code>InferOpt.sample_perturbations</code></a></li><li><a href="#InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R&lt;:Real"><code>InferOpt.shannon_entropy</code></a></li><li><a href="#InferOpt.simplex_projection_and_support-Tuple{AbstractVector}"><code>InferOpt.simplex_projection_and_support</code></a></li><li><a href="#InferOpt.zero_one_loss-Tuple{AbstractArray, AbstractArray}"><code>InferOpt.zero_one_loss</code></a></li><li><a href="#InferOpt.zero_one_loss_maximizer-Union{Tuple{R}, Tuple{AbstractVector, AbstractVector{R}, Any}} where R&lt;:Real"><code>InferOpt.zero_one_loss_maximizer</code></a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../losses/">« Losses</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Wednesday 5 July 2023 09:10">Wednesday 5 July 2023</span>. Using Julia version 1.9.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
