var documenterSearchIndex = {"docs":
[{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"EditURL = \"https://github.com/axelparmentier/InferOpt.jl/blob/main/test/tutorial.jl\"","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#Context","page":"Tutorial","title":"Context","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let us imagine that we observe the itineraries chosen by a public transport user in several different networks, and that we want to understand their decision-making process (a.k.a. recover their utility function).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"More precisely, each point in our dataset consists in:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"a graph G\na shortest path P from the top left to the bottom right corner","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We don't know the true costs that were used to compute the shortest path, but we can exploit a set of features to approximate these costs. The question is: how should we combine these features?","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We will use InferOpt to learn the appropriate weights, so that we may propose relevant paths to the user in the future.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Flux\nusing Graphs\nusing GridGraphs\nusing InferOpt\nusing InferOpt.Testing\nusing LinearAlgebra\nusing ProgressMeter\nusing Random\nusing Statistics\nusing Test\nusing UnicodePlots\n\nRandom.seed!(63);\nnothing #hide","category":"page"},{"location":"tutorial/#Grid-graphs","page":"Tutorial","title":"Grid graphs","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For the purposes of this tutorial, we consider grid graphs, as implemented in GridGraphs.jl. In such graphs, each vertex corresponds to a couple of coordinates (i j), where 1 leq i leq h and 1 leq j leq w.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To ensure acyclicity, we only allow the user to move right, down or both. Since the cost of a move is defined as the cost of the arrival vertex, any grid graph is entirely characterized by its cost matrix theta in mathbbR^h times w.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"h, w = 50, 100\ng = AcyclicGridGraph(rand(h, w));\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For convenience, GridGraphs.jl also provides custom functions to compute shortest paths efficiently. Let us see what those paths look like.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"p = path_to_matrix(g, grid_topological_sort(g, 1, nv(g)));\nspy(p)","category":"page"},{"location":"tutorial/#Dataset","page":"Tutorial","title":"Dataset","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As announced, we do not know the cost of each vertex, only a set of relevant features. Let us assume that the user combines them using a shallow neural network.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"nb_features = 5\n\ntrue_encoder = Chain(Dense(nb_features, 1), dropfirstdim);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The true vertex costs computed from this encoding are then used within shortest path computations. To be consistent with the literature, we frame this problem as a linear maximization problem, which justifies the change of sign in front of theta.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function linear_maximizer(θ)\n    g = AcyclicGridGraph(-θ)\n    path = grid_topological_sort(g, 1, nv(g))\n    return path_to_matrix(g, path)\nend;\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We now have everything we need to build our dataset.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"nb_instances = 30\n\nX_train = [randn(nb_features, h, w) for n in 1:nb_instances];\nθ_train = [true_encoder(x) for x in X_train];\nY_train = [linear_maximizer(θ) for θ in θ_train];\nnothing #hide","category":"page"},{"location":"tutorial/#Learning","page":"Tutorial","title":"Learning","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We create a trainable model with the same structure as the true encoder but another set of randomly-initialized weights.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"initial_encoder = Chain(Dense(nb_features, 1), dropfirstdim);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here is the crucial part where InferOpt intervenes: the choice of a clever loss function that enables us to","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"differentiate through the shortest path maximizer, even though it is a combinatorial operation\nevaluate the quality of our model based on the paths that it recommends","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"regularized_predictor = PerturbedNormal(linear_maximizer; ε=1.0, M=5);\nloss = FenchelYoungLoss(regularized_predictor);\nnothing #hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The regularized predictor is just a thin wrapper around our linear_maximizer, but with a very different behavior:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"p_regularized = regularized_predictor(θ_train[1]);\nspy(p_regularized)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Instead of choosing just one path, it spreads over several possible paths, allowing its output to change smoothly as theta varies. Thanks to this smoothing, we can now train our model with a standard gradient optimizer.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"encoder = deepcopy(initial_encoder)\nopt = ADAM();\nlosses = Float64[]\nfor epoch in 1:200\n    l = 0.\n    for (x, y) in zip(X_train, Y_train)\n        grads = gradient(Flux.params(encoder)) do\n            l += loss(encoder(x), y)\n        end\n        Flux.update!(opt, Flux.params(encoder), grads)\n    end\n    push!(losses, l)\nend;\nnothing #hide","category":"page"},{"location":"tutorial/#Results","page":"Tutorial","title":"Results","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Since the Fenchel-Young loss is convex, it is no wonder that optimization worked like a charm.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"lineplot(losses, xlabel=\"Epoch\", ylabel=\"Loss\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To assess performance, we can compare the learned weights with their true (hidden) values","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"learned_weight = encoder[1].weight / norm(encoder[1].weight)\ntrue_weight = true_encoder[1].weight / norm(true_encoder[1].weight)\n@info \"Parameter error\" learned_weight true_weight","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We are quite close to recovering the exact user weights. But in reality, it doesn't matter as much as our ability to provide accurate path predictions. Let us therefore compare our predictions with the actual paths on the training set.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Y_train_pred = [linear_maximizer(encoder(x)) for x in X_train];\n\ntrain_error = mean(\n    normalized_hamming_distance(y, y_pred) for (y, y_pred) in zip(Y_train, Y_train_pred)\n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Not too bad, at least compared with our random initial encoder.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Y_train_pred_initial = [linear_maximizer(initial_encoder(x)) for x in X_train];\n\ntrain_error_initial = mean(\n    normalized_hamming_distance(y, y_pred) for (y, y_pred) in zip(Y_train, Y_train_pred_initial)\n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This is definitely a success. Of course in real prediction settings we should measure performance on a test set as well. This is left as an exercise to the reader.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This page was generated using Literate.jl.","category":"page"},{"location":"api/#API-Reference","page":"API reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Index","page":"API reference","title":"Index","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"","category":"page"},{"location":"api/#Docstrings","page":"API reference","title":"Docstrings","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [InferOpt]","category":"page"},{"location":"api/#InferOpt.AbstractPerturbed","page":"API reference","title":"InferOpt.AbstractPerturbed","text":"AbstractPerturbed{F}\n\nDifferentiable perturbation of a black-box optimizer.\n\nSubtypes:\n\nPerturbedNormal{F}\nPerturbedLogNormal{F}\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.FenchelYoungLoss","page":"API reference","title":"InferOpt.FenchelYoungLoss","text":"FenchelYoungLoss{P}\n\nFenchel-Young loss associated with a given regularized prediction function.\n\nFields\n\npredictor::P: prediction function, usually of the form θ ⟼ ŷ(θ) = argmax ⟨θ,y⟩ - Ω(y)\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.GeneralStructuredLoss","page":"API reference","title":"InferOpt.GeneralStructuredLoss","text":"GeneralStructuredLoss{F1, F2}\n\ndelta_loss(y, y_true)\nmaximizer(θ, α, y_true) = argmax_y (delta_loss(y, y_true) + α ⟨θ,  y - y_true⟩)\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.Interpolation","page":"API reference","title":"InferOpt.Interpolation","text":"Interpolation{F}\n\nPiecewise-linear interpolation of a black-box optimizer.\n\nFields\n\nmaximizer::F: underlying argmax function\nλ::Float64: smoothing parameter (smaller = more faithful approximation, larger = more informative gradients)\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.IsRegularizedPrediction","page":"API reference","title":"InferOpt.IsRegularizedPrediction","text":"IsRegularizedPrediction{P}\n\nTrait-based interface for regularized prediction functions of the form θ ⟼ ŷ(θ) = argmax ⟨θ,y⟩ - Ω(y).\n\nFor a type P to comply with this interface, the following methods must exist:\n\n(prediction::P)(θ)\ncompute_regularization(prediction::P, y)\n\nWe provide some special cases with explicit formulae, such as:\n\none_hot_argmax\nsoftmax\nsparsemax\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.IsStructuredLossFunction","page":"API reference","title":"InferOpt.IsStructuredLossFunction","text":"IsStructuredLossFunction{L}\n\nTrait-based interface for structured loss functions of the form `(y, ytrue) ⟼ l(y, ytrue). You need a StructuredLossFunction in order build StructuredSVM losses.\n\nFor a type L to comply with this interface, the following methods must exist:\n\n(loss::L)(y, y_true)\ncompute_maximizer(loss::L, θ, α, y_true)\nit should return the following : argmaxy (loss(y, ytrue) + α ⟨θ,  y - y_true⟩)\nuseful to compute the gradient of an associated SSVM loss.\n\nWe provide some special cases with explicit formulae, such as:\n\nZeroOneLoss\n\nWe also provide a generic wrapper GeneralStructuredLoss to build your own StructuredLossFunction.\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.PerturbedCost","page":"API reference","title":"InferOpt.PerturbedCost","text":"PerturbedCost{F,P<:AbstractPerturbed{F},C}\n\nComposition of a differentiable perturbed black-box optimizer with an arbitrary cost function. Designed for direct regret minimization (learning by experience).\n\nFields\n\nperturbed::P: underlying AbstractPerturbed{F} wrapper\ncost::C: a real-valued function taking a vector y and some kwargs as inputs\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.PerturbedLogNormal","page":"API reference","title":"InferOpt.PerturbedLogNormal","text":"PerturbedLogNormal{F}\n\nDifferentiable log-normal perturbation of a black-box optimizer: the input undergoes θ -> exp[εZ - ε²/2] * θ where Z ∼ N(0, 1).\n\nFields\n\nmaximizer::F: underlying argmax function\nε::Float64: noise scaling parameter\nM::Int: number of noise samples for Monte-Carlo computations\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.PerturbedNormal","page":"API reference","title":"InferOpt.PerturbedNormal","text":"PerturbedNormal{F}\n\nDifferentiable normal perturbation of a black-box optimizer: the input undergoes θ -> θ + εZ where Z ∼ N(0, 1).\n\nFields\n\nmaximizer::F: underlying argmax function\nε::Float64: noise scaling parameter\nM::Int: number of noise samples for Monte-Carlo computations\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.SPOPlusLoss","page":"API reference","title":"InferOpt.SPOPlusLoss","text":"SPOPlusLoss{F}\n\nConvex surrogate of the SPO loss.\n\nFields\n\nmaximizer::F: linear maximizer function of the form θ ⟼ ŷ(θ) = argmax ⟨θ,y⟩\nα::Float64: convexification parameter\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.StructuredSVMLoss","page":"API reference","title":"InferOpt.StructuredSVMLoss","text":"StructuredSVMLoss{L, R<:Real}\n\nL should satisfy the IsStructuredLossFunction trait.\n\nSSVMloss(θ, y_true) = max_y (l(y, y_true) + α [⟨θ, y⟩ - ⟨θ, y_true⟩])\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.ZeroOneLoss","page":"API reference","title":"InferOpt.ZeroOneLoss","text":"ZeroOneLoss\n\nZero-one loss for multiclass classification (ZeroOneLoss(y, y_true) equals 0 if y == y_true, else 1).\n\n\n\n\n\n","category":"type"},{"location":"api/#InferOpt.half_square_norm-Tuple{AbstractArray{<:Real}}","page":"API reference","title":"InferOpt.half_square_norm","text":"half_square_norm(x)\n\nCompute the squared Euclidean norm of x and divide it by 2.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.isproba-Tuple{Real}","page":"API reference","title":"InferOpt.isproba","text":"isproba(x)\n\nCheck whether x ∈ [0,1].\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.isprobadist-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R<:Real","page":"API reference","title":"InferOpt.isprobadist","text":"isprobadist(p)\n\nCheck whether the elements of p are nonnegative and sum to 1.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.one_hot_argmax-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R<:Real","page":"API reference","title":"InferOpt.one_hot_argmax","text":"one_hot_argmax(z)\n\nOne-hot encoding of the argmax function.\n\nCorresponds to regularized prediction on the probability simplex with zero penalty.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.positive_part-Tuple{Any}","page":"API reference","title":"InferOpt.positive_part","text":"positive_part(x)\n\nCompute max(x,0).\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.shannon_entropy-Union{Tuple{AbstractVector{R}}, Tuple{R}} where R<:Real","page":"API reference","title":"InferOpt.shannon_entropy","text":"shannon_entropy(p)\n\nCompute the Shannon entropy of a probability distribution: H(p) = -∑ pᵢlog(pᵢ).\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.simplex_projection_and_support-Tuple{AbstractVector{<:Real}}","page":"API reference","title":"InferOpt.simplex_projection_and_support","text":"simplex_projection_and_support(z)\n\nCompute the Euclidean projection p of z on the probability simplex (also called sparsemax), and the indicators s of its support.\n\nSee https://arxiv.org/abs/1602.02068.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.softmax-Tuple{AbstractVector{<:Real}}","page":"API reference","title":"InferOpt.softmax","text":"softmax(θ)\n\nSoftmax function s(z) = (e^zᵢ / ∑ e^zⱼ)ᵢ.\n\nCorresponds to regularized prediction on the probability simplex with entropic penalty.\n\n\n\n\n\n","category":"method"},{"location":"api/#InferOpt.sparsemax-Tuple{AbstractVector{<:Real}}","page":"API reference","title":"InferOpt.sparsemax","text":"sparsemax(z)\n\nProject the vector z onto the probability simplex Δ in time O(d log d).\n\nCorresponds to regularized prediction on the probability simplex with square norm penalty.\n\n\n\n\n\n","category":"method"},{"location":"math/#Mathematical-background","page":"Mathematical background","title":"Mathematical background","text":"","category":"section"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"Here we describe the theoretical framework in which InferOpt.jl operates. Our goal is make machine learning models more expressive by allowing them to use combinatorial optimization algorithms as layers.","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"For a broader perspective on the interactions between machine learning and combinatorial optimization, please refer to the following review papers:","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"Machine learning for combinatorial optimization: A methodological tour d’horizon","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"End-to-End Constrained Optimization Learning: A Survey","category":"page"},{"location":"math/#General-setting","page":"Mathematical background","title":"General setting","text":"","category":"section"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"Let mathcalX be a set of instances, in a very broad sense. For each instance x in mathcalX, we want to predict an output z in mathcalZ(x) which minimizes a given cost function c(z). The cost may also depend on x, but we omit this dependency for notational simplicity:","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"min_z in mathcalZ(x) c(z)","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"In many real-world applications, the set of feasible predictions is combinatorially large: well-known examples include rankings, paths, flows, etc. To tackle such scenarios, a common approach in the literature is to delegate all combinatorial aspects to a surrogate optimization problem, which is typically a Linear Program (LP).","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"Let us therefore introduce a combinatorially large but structured set mathcalY(x). By structured, we mean that there is a natural embedding of mathcalY(x) in mathbbR^d(x), and that we have efficient algorithms to solve the following problem: [1]","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"[1]: We switched to maximization here to ensure consistency with the literature on structured learning.","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"max_y in mathcalY(x) theta^T y tagLP","category":"page"},{"location":"math/#Structured-learning-pipeline","page":"Mathematical background","title":"Structured learning pipeline","text":"","category":"section"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"The main purpose of InferOpt.jl is to integrate the optimization problem (LP) into a learning pipeline such as this one:","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"xrightarrowtextInstancex in mathcalX\nfboxEncoder varphi_w\nxrightarrowtextCost vectortheta in mathbbR^d(x)\nfboxOptimizer\nxrightarrowtextSolutiony in mathcalY(x)\nfboxDecoder psi\nxrightarrowtextOutputz in mathcalZ(x)","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"What is going on here?","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"The encoder varphi_w transforms an instance x in mathcalX into a cost vector theta = varphi_w(x) in mathbbR^d(x). It can be any machine learning algorithm parameterized by a set of weights w, like a GLM or neural network.\nThe optimizer solves (LP) and returns an optimal solution haty(theta) in argmax_y in mathcalY(x) theta^T y.\nThe decoder psi turns the solution haty(theta) into an output hatz in mathcalZ(x). It is usually a handcrafted repair or local search heuristic with no learnable parameters.","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"Our goal is to learn the encoder weights w based on previous instances, so that we may solve future instances by plugging them into our pipeline.","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"In many of the methods we describe, the decoder either doesn't exist or doesn't play a central role, which is why we ignore it in what follows by simply taking mathcalY(x) = mathcalZ(x).","category":"page"},{"location":"math/#Learning-by-experience","page":"Mathematical background","title":"Learning by experience","text":"","category":"section"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"Let us denote by f_w = haty circ varphi_w our prediction pipeline. A natural instinct would be to find the weights w that minimize the empirical regret, and prevent overfitting with a weight regularization term g(w):","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"R_n(w) = frac1n sum_i=1^n c(f_w(x_i)) + g(w)","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"Directly minimizing R_n(w) can be referred to as learning by experience, since the algorithm is only guided by the knowledge of past instances x_i.","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"Unfortunately, the function R_n depends on c (which can be an expensive black box) and f_w (which is, in general, piecewise-constant). As a result, it is not easy to handle directly, and we often need additional guidance.","category":"page"},{"location":"math/#Learning-by-imitation","page":"Mathematical background","title":"Learning by imitation","text":"","category":"section"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"For each instance x_i in mathcalX, our training set may also contain some target xi_i in Xi(x_i) that orients us towards desirable behavior. Learning by imitation is cheaper since expensive black box computations can happen offline, outside of the training loop.","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"There are two main cases:","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"When xi_i = bary_i is a precomputed optimal solution (as in Structured SVM and Fenchel-Young losses).\nWhen xi_i = bartheta_i is the true cost vector, from which we can recover bary_i in argmax_y in mathcalY(x) langle bartheta_i y rangle (as in Smart \"Predict, then Optimize\").","category":"page"},{"location":"math/","page":"Mathematical background","title":"Mathematical background","text":"The cost function is usually adapted to accept the target as an additional argument, and possibly convexified.","category":"page"},{"location":"","page":"Home","title":"Home","text":"EditURL = \"https://github.com/axelparmentier/InferOpt.jl/blob/main/README.md\"","category":"page"},{"location":"#InferOpt.jl","page":"Home","title":"InferOpt.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Dev) (Image: Build Status) (Image: Coverage) (Image: Code Style: Blue)","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"InferOpt.jl is a toolbox for using optimization algorithms within inference and learning tasks.","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install this package, open a Julia Pkg REPL and run the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add https://github.com/axelparmentier/InferOpt.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is in a very early development stage, so use it at your own risk!","category":"page"},{"location":"implementation/#Implementation","page":"Implementation","title":"Implementation","text":"","category":"section"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"Here we describe the technical details of the InferOpt.jl codebase.","category":"page"},{"location":"implementation/#Differentiable-optimization-layers","page":"Implementation","title":"Differentiable optimization layers","text":"","category":"section"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"In the Mathematical background, we saw that our package provides a principled way to approximate combinatorial problems with machine learning. More specifically, we implement several ways to convert combinatorial problems into differentiable layers of a machine learning pipeline.","category":"page"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"Since we want our package to be as generic as possible, we don't make any assumptions on the kind of algorithm used to solve these combinatorial problems. We only ask the user to provide a function called maximizer, which takes theta as argument and returns a solution haty(theta) in argmax_y in mathcalC theta^T y.","category":"page"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"This function is then wrapped into a callable Julia struct that can be used (for instance) within neural networks from the Flux.jl library.","category":"page"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"Flux: Elegant machine learning with Julia","category":"page"},{"location":"implementation/#Defining-chain-rules","page":"Implementation","title":"Defining chain rules","text":"","category":"section"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"To achieve this goal, we leverage Julia's Automatic Differentiation (AD) ecosystem, which revolves around the ChainRules.jl package.","category":"page"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"See the paper below for an overview of this ecosystem:","category":"page"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"AbstractDifferentiation.jl: Backend-Agnostic Differentiable Programming in Julia","category":"page"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"If you need a refresher on forward and reverse-mode AD, the following survey is a good starting point:","category":"page"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"Automatic Differentiation in Machine Learning: a Survey","category":"page"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"In machine learning (especially deep learning), reverse-mode AD is by far the most common. Therefore, as soon as we define a new type of layer, we must make it possible to compute the backward pass through this layer. In other words, for each function b = f(a), we need to implement a \"pullback function\" that takes a perturbation delta_b and returns the associated perturbation delta_a = delta_b fracmathrmdbmathrmda. In case the function f is not differentiable, returning a subgradient is sufficient.","category":"page"},{"location":"implementation/","page":"Implementation","title":"Implementation","text":"See the ChainRules.jl documentation for more details.","category":"page"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Here we describe each approach available in InferOpt.jl.","category":"page"},{"location":"algorithms/#Combinatorial-problems-as-layers","page":"Algorithms","title":"Combinatorial problems as layers","text":"","category":"section"},{"location":"algorithms/#Linear-formulation","page":"Algorithms","title":"Linear formulation","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"As we stated in the Mathematical background, our package is centered around the integration of LP layers such as","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"theta longmapsto haty(theta) = argmax_y in mathcalY theta^T y tagLP","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"into machine learning pipelines. Here, theta is a cost vector (obtained as the output of the encoder varphi_w), while mathcalY is a finite subset of mathbbR^d. Since the optimum of an LP is always reached at a vertex of the feasible polytope, we can start by replacing mathcalY with its convex hull mathcalC = mathrmconv(mathcalY).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Note that the set of feasible solutions mathcalY and its convex hull mathcalC may depend on the instance x. In that case, we use the notations mathcalY(x) and mathcalC(x), but the exposition doesn't change.","category":"page"},{"location":"algorithms/#Implementation-doesn't-matter","page":"Algorithms","title":"Implementation doesn't matter","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Importantly, our framework does not constrain the actual procedure used to find a solution haty(theta). As long as the problem we solve corresponds to the maximization of a linear function over a convex polytope mathcalC, anything is fair game, and we do not care about the implementation details.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Example 1: If we consider a Mixed Integer Linear Program (MILP), the convex hull of the integer solutions often cannot be expressed in a concise way. In that case, we will most likely use a MILP solver on mathcalY instead of an LP solver on mathcalC. Still, the problem can be described as a maximization over the continuous polytope mathcalC.\nExample 2: In some applications, we don't even have to rely on mathematical programming solvers such as CPLEX or Gurobi. For instance, Dijkstra's algorithm for shortest paths or the Edmonds-Karp algorithm for maximum flows can also be used to tackle LPs with specific structure.","category":"page"},{"location":"algorithms/#The-problem-with-differentiability","page":"Algorithms","title":"The problem with differentiability","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Let us suppose that the problem (LP) is one of many layers in a (deep) learning pipeline. To lean the parameters of the other layers, we would like to use a gradient algorithm, which requires the whole pipeline to be differentiable. Unfortunately, the argmin of an LP is a piecewise-constant function, able to jump discontinuously between polytope vertices with very small shifts in the cost vector theta.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Therefore, a major contribution of our package consists in a toolbox for constructing differentiable approximations of discrete optimization layers.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"We now present the catalogue of methods available in InferOpt.jl, along with the differentiation formulae. In what follows, the pullback function computes vector-jacobian products (see Defining chain rules to understand why that is a central notion in differentiable programming).","category":"page"},{"location":"algorithms/#Differentiating-through-an-argmax","page":"Algorithms","title":"Differentiating through an argmax","text":"","category":"section"},{"location":"algorithms/#Piecewise-linear-interpolation","page":"Algorithms","title":"Piecewise-linear interpolation","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A first option is to construct a piecewise-linear interpolation, whose distance from the piecewise-constant argmax is controlled by a smoothing parameter lambda. Here is the formula for the backward pass:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"textttpullback(delta_y) = frac1lambda(haty(theta + lambda delta_y) - haty(theta))","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Differentiation of Blackbox Combinatorial Solvers","category":"page"},{"location":"algorithms/#Regularized-prediction","page":"Algorithms","title":"Regularized prediction","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Another solution is to regularize the predictor haty(theta) = argmax_y in mathcalC theta^T y using a regularization function Omega on the output space. This is expressed as follows:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"haty_Omega(theta) = argmax_y in mathcalC theta^T y - Omega(y)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Learning with Fenchel-Young Losses","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A special case of this approach is regularization by stochastic perturbation. Let varepsilon  0 and Z be a random vector with negative log-density nu(z): we can define the perturbed optimizer","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"haty_varepsilon(theta) = mathbbE_Z big argmax_y in mathcalC (theta + varepsilon Z)^T y big","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"In this setting, the function Omega has no explicit expression However, we have a formula for the Jacobian of haty_varepsilon(theta) with respect to theta:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"J_theta haty_varepsilon(theta) = mathbbE_Z big haty(theta + varepsilon Z) nabla nu(Z)^T  varepsilon big","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Therefore, we can compute a stochastic pullback function using samples Z_1Z_M (which must the same as in the forward pass):","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"textttpullback(delta_y) = frac1varepsilon M sum_i=1^M bigdelta_y^T haty(theta + varepsilon Z_i)big nabla nu(Z_i)^T","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Learning with Differentiable Perturbed Optimizers","category":"page"},{"location":"algorithms/#Implicit-differentiation","page":"Algorithms","title":"Implicit differentiation","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"In this paragraph, the optimizer haty(theta) in arg max_y in mathcalC f(y theta) is not necessarily an LP. However, we assume that every optimal solution must satisfy the following abstract conditions:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"F(haty(theta) theta) = 0","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The implicit function theorem then gives the following relation between the jacobian matrices of haty, F(cdot theta) and F(y cdot):","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"underbrace- J_y F(haty(theta) theta)_A cdot J_theta haty(theta) = underbraceJ_theta F(haty(theta) theta)_B","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The matrices A and B can be computed with automatic differentiation, but as it turns out, we don't need to store them entirely to compute vector-jacobian products.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Efficient and Modular Implicit Differentiation","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"note: Stay tuned!\nThis will soon be implemented thanks to the recent package ImplicitDifferentiation.jl.","category":"page"},{"location":"algorithms/#Evaluating-our-predictions","page":"Algorithms","title":"Evaluating our predictions","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Another crucial ingredient is a loss function that takes the structure of the problem into account. Since any loss ell(theta) is a real-valued function, as soon as we have a subgradient g in partial ell(theta), we can automatically define a pullback as follows:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"textttpullback(delta_ell) = delta_ell g","category":"page"},{"location":"algorithms/#Smart-\"Predict,-then-Optimize\"","page":"Algorithms","title":"Smart \"Predict, then Optimize\"","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Even when we know the true cost vector bartheta, we may not simply want to minimize the error lVert theta - bartheta rVert. What we are actually interested in is the impact of this error on the downstream optimization problem. This is accurately measured by the SPO loss, of which the SPO+ loss is a convex surrogate:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ell^SPO+(theta bartheta) = (2theta - bartheta)^Thaty(2 theta - bartheta) + (bartheta - 2theta)^T haty(bartheta)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A subgradient with respect to theta is given by","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"2haty(2 theta - bartheta) - 2haty(bartheta) in partial_theta ell^SPO+(theta bartheta)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Smart \"Predict, then Optimize\"","category":"page"},{"location":"algorithms/#Structured-SVM","page":"Algorithms","title":"Structured SVM","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Suppose we define a \"distance\" function Delta(baryy) on the output space. The associated Structured SVM loss is given by","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ell^SSVM(theta bary) = max_y in mathcalC Delta(baryy) + theta^T(y - bary)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Structured learning and prediction in computer vision, Chapter 6","category":"page"},{"location":"algorithms/#Fenchel-Young-losses","page":"Algorithms","title":"Fenchel-Young losses","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"As soon as we have a regularized predictor, Fenchel-Young losses provide a systematic way to evaluate prediction quality in structured settings:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ell^FY(theta bary)\n= Omega^*(theta) + Omega(bary) - theta^T bary\n= max_y in mathcalC left( theta^T y - Omega(y) right) - left( theta^T bary - Omega(bary) right)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A subgradient with respect to theta is given by the residual:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"    haty_Omega(theta) - bary in partial_theta ell^FY(theta bary)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Learning with Fenchel-Young Losses","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"In the case of stochastic perturbation, we cannot compute the full Fenchel-Young loss since Omega(bary) has no explicit formula. However, we can still optimize it without that term since it does not depend on theta:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ell^FYP(theta bary) = mathbbEbigmax_yinmathcalC (theta + varepsilon Z)^T y big - theta^T bary","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The subgradient expression thus remains unchanged.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Learning with Differentiable Perturbed Optimizers","category":"page"}]
}
